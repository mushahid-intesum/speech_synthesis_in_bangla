{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eea754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, field, replace\n",
    "from itertools import chain\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torchaudio\n",
    "from coqpit import Coqpit\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "from torch import nn\n",
    "from torch.cuda.amp.autocast_mode import autocast\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from trainer.torch import DistributedSampler, DistributedSamplerWrapper\n",
    "from trainer.trainer_utils import get_optimizer, get_scheduler\n",
    "\n",
    "from TTS.tts.configs.shared_configs import CharactersConfig\n",
    "from TTS.tts.datasets.dataset import TTSDataset, _parse_sample\n",
    "from TTS.tts.layers.glow_tts.duration_predictor import DurationPredictor\n",
    "from TTS.tts.layers.vits.discriminator import VitsDiscriminator\n",
    "from TTS.tts.layers.vits.networks import PosteriorEncoder, ResidualCouplingBlocks, TextEncoder\n",
    "from TTS.tts.layers.vits.stochastic_duration_predictor import StochasticDurationPredictor\n",
    "from TTS.tts.models.base_tts import BaseTTS\n",
    "from TTS.tts.utils.fairseq import rehash_fairseq_vits_checkpoint\n",
    "from TTS.tts.utils.helpers import generate_path, maximum_path, rand_segments, segment, sequence_mask\n",
    "from TTS.tts.utils.languages import LanguageManager\n",
    "from TTS.tts.utils.speakers import SpeakerManager\n",
    "from TTS.tts.utils.synthesis import synthesis\n",
    "from TTS.tts.utils.text.characters import BaseCharacters, BaseVocabulary, _characters, _pad, _phonemes, _punctuations\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.tts.utils.visual import plot_alignment\n",
    "from TTS.utils.io import load_fsspec\n",
    "from TTS.utils.samplers import BucketBatchSampler\n",
    "from TTS.vocoder.models.hifigan_generator import HifiganGenerator\n",
    "from TTS.vocoder.utils.generic_utils import plot_results\n",
    "\n",
    "import epitran\n",
    "from text.bn_phonemiser import bangla_text_normalize, replace_number_with_text\n",
    "\n",
    "\n",
    "##############################\n",
    "# IO / Feature extraction\n",
    "##############################\n",
    "\n",
    "# pylint: disable=global-statement\n",
    "hann_window = {}\n",
    "mel_basis = {}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def weights_reset(m: nn.Module):\n",
    "    # check if the current module has reset_parameters and if it is reset the weight\n",
    "    reset_parameters = getattr(m, \"reset_parameters\", None)\n",
    "    if callable(reset_parameters):\n",
    "        m.reset_parameters()\n",
    "\n",
    "\n",
    "def get_module_weights_sum(mdl: nn.Module):\n",
    "    dict_sums = {}\n",
    "    for name, w in mdl.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            value = w.data.sum().item()\n",
    "            dict_sums[name] = value\n",
    "    return dict_sums\n",
    "\n",
    "\n",
    "def load_audio(file_path):\n",
    "    \"\"\"Load the audio file normalized in [-1, 1]\n",
    "\n",
    "    Return Shapes:\n",
    "        - x: :math:`[1, T]`\n",
    "    \"\"\"\n",
    "    x, sr = torchaudio.load(file_path)\n",
    "    assert (x > 1).sum() + (x < -1).sum() == 0\n",
    "    return x, sr\n",
    "\n",
    "\n",
    "def _amp_to_db(x, C=1, clip_val=1e-5):\n",
    "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
    "\n",
    "\n",
    "def _db_to_amp(x, C=1):\n",
    "    return torch.exp(x) / C\n",
    "\n",
    "\n",
    "def amp_to_db(magnitudes):\n",
    "    output = _amp_to_db(magnitudes)\n",
    "    return output\n",
    "\n",
    "\n",
    "def db_to_amp(magnitudes):\n",
    "    output = _db_to_amp(magnitudes)\n",
    "    return output\n",
    "\n",
    "\n",
    "def wav_to_spec(y, n_fft, hop_length, win_length, center=False):\n",
    "    \"\"\"\n",
    "    Args Shapes:\n",
    "        - y : :math:`[B, 1, T]`\n",
    "\n",
    "    Return Shapes:\n",
    "        - spec : :math:`[B,C,T]`\n",
    "    \"\"\"\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    if torch.min(y) < -1.0:\n",
    "        print(\"min value is \", torch.min(y))\n",
    "    if torch.max(y) > 1.0:\n",
    "        print(\"max value is \", torch.max(y))\n",
    "\n",
    "    global hann_window\n",
    "    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n",
    "    wnsize_dtype_device = str(win_length) + \"_\" + dtype_device\n",
    "    if wnsize_dtype_device not in hann_window:\n",
    "        hann_window[wnsize_dtype_device] = torch.hann_window(win_length).to(dtype=y.dtype, device=y.device)\n",
    "\n",
    "    y = torch.nn.functional.pad(\n",
    "        y.unsqueeze(1),\n",
    "        (int((n_fft - hop_length) / 2), int((n_fft - hop_length) / 2)),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    spec = torch.stft(\n",
    "        y,\n",
    "        n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        window=hann_window[wnsize_dtype_device],\n",
    "        center=center,\n",
    "        pad_mode=\"reflect\",\n",
    "        normalized=False,\n",
    "        onesided=True,\n",
    "        return_complex=False,\n",
    "    )\n",
    "\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "    return spec\n",
    "\n",
    "\n",
    "def spec_to_mel(spec, n_fft, num_mels, sample_rate, fmin, fmax):\n",
    "    \"\"\"\n",
    "    Args Shapes:\n",
    "        - spec : :math:`[B,C,T]`\n",
    "\n",
    "    Return Shapes:\n",
    "        - mel : :math:`[B,C,T]`\n",
    "    \"\"\"\n",
    "    global mel_basis\n",
    "    dtype_device = str(spec.dtype) + \"_\" + str(spec.device)\n",
    "    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n",
    "    if fmax_dtype_device not in mel_basis:\n",
    "        mel = librosa_mel_fn(sr=sample_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
    "        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n",
    "    mel = torch.matmul(mel_basis[fmax_dtype_device], spec)\n",
    "    mel = amp_to_db(mel)\n",
    "    return mel\n",
    "\n",
    "\n",
    "def wav_to_mel(y, n_fft, num_mels, sample_rate, hop_length, win_length, fmin, fmax, center=False):\n",
    "    \"\"\"\n",
    "    Args Shapes:\n",
    "        - y : :math:`[B, 1, T]`\n",
    "\n",
    "    Return Shapes:\n",
    "        - spec : :math:`[B,C,T]`\n",
    "    \"\"\"\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    if torch.min(y) < -1.0:\n",
    "        print(\"min value is \", torch.min(y))\n",
    "    if torch.max(y) > 1.0:\n",
    "        print(\"max value is \", torch.max(y))\n",
    "\n",
    "    global mel_basis, hann_window\n",
    "    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n",
    "    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n",
    "    wnsize_dtype_device = str(win_length) + \"_\" + dtype_device\n",
    "    if fmax_dtype_device not in mel_basis:\n",
    "        mel = librosa_mel_fn(sr=sample_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
    "        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=y.dtype, device=y.device)\n",
    "    if wnsize_dtype_device not in hann_window:\n",
    "        hann_window[wnsize_dtype_device] = torch.hann_window(win_length).to(dtype=y.dtype, device=y.device)\n",
    "\n",
    "    y = torch.nn.functional.pad(\n",
    "        y.unsqueeze(1),\n",
    "        (int((n_fft - hop_length) / 2), int((n_fft - hop_length) / 2)),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    spec = torch.stft(\n",
    "        y,\n",
    "        n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        window=hann_window[wnsize_dtype_device],\n",
    "        center=center,\n",
    "        pad_mode=\"reflect\",\n",
    "        normalized=False,\n",
    "        onesided=True,\n",
    "        return_complex=False,\n",
    "    )\n",
    "\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n",
    "    spec = amp_to_db(spec)\n",
    "    return spec\n",
    "\n",
    "\n",
    "#############################\n",
    "# CONFIGS\n",
    "#############################\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VitsAudioConfig(Coqpit):\n",
    "    fft_size: int = 1024\n",
    "    sample_rate: int = 22050\n",
    "    win_length: int = 1024\n",
    "    hop_length: int = 256\n",
    "    num_mels: int = 80\n",
    "    mel_fmin: int = 0\n",
    "    mel_fmax: int = None\n",
    "\n",
    "\n",
    "##############################\n",
    "# DATASET\n",
    "##############################\n",
    "\n",
    "\n",
    "def get_attribute_balancer_weights(items: list, attr_name: str, multi_dict: dict = None):\n",
    "    \"\"\"Create inverse frequency weights for balancing the dataset.\n",
    "    Use `multi_dict` to scale relative weights.\"\"\"\n",
    "    attr_names_samples = np.array([item[attr_name] for item in items])\n",
    "    unique_attr_names = np.unique(attr_names_samples).tolist()\n",
    "    attr_idx = [unique_attr_names.index(l) for l in attr_names_samples]\n",
    "    attr_count = np.array([len(np.where(attr_names_samples == l)[0]) for l in unique_attr_names])\n",
    "    weight_attr = 1.0 / attr_count\n",
    "    dataset_samples_weight = np.array([weight_attr[l] for l in attr_idx])\n",
    "    dataset_samples_weight = dataset_samples_weight / np.linalg.norm(dataset_samples_weight)\n",
    "    if multi_dict is not None:\n",
    "        # check if all keys are in the multi_dict\n",
    "        for k in multi_dict:\n",
    "            assert k in unique_attr_names, f\"{k} not in {unique_attr_names}\"\n",
    "        # scale weights\n",
    "        multiplier_samples = np.array([multi_dict.get(item[attr_name], 1.0) for item in items])\n",
    "        dataset_samples_weight *= multiplier_samples\n",
    "    return (\n",
    "        torch.from_numpy(dataset_samples_weight).float(),\n",
    "        unique_attr_names,\n",
    "        np.unique(dataset_samples_weight).tolist(),\n",
    "    )\n",
    "\n",
    "\n",
    "class VitsDataset(TTSDataset):\n",
    "    def __init__(self, model_args, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pad_id = self.tokenizer.characters.pad_id\n",
    "        self.model_args = model_args\n",
    "\n",
    "        self.bn_phonemizer = epitran.Epitran('ben-Beng-east')\n",
    "\n",
    "        self.ipa_dict = {'É–Ì¤': 0,\n",
    "                        'kÊ°': 1,\n",
    "                        'n': 2,\n",
    "                        'm': 3,\n",
    "                        'É¡Ì¤': 4,\n",
    "                        'Êƒ': 5,\n",
    "                        'bÌ¤': 6,\n",
    "                        'dÌªÌ¤': 7,\n",
    "                        'Å‹': 8,\n",
    "                        'pÊ°': 9,\n",
    "                        'É½Ì¤': 10,\n",
    "                        'k': 11,\n",
    "                        'a': 12,\n",
    "                        'b': 13,\n",
    "                        'r': 14,\n",
    "                        'ÊˆÊ°': 15,\n",
    "                        'V': 16,\n",
    "                        'É–': 17,\n",
    "                        'tÌªÊ°': 18,\n",
    "                        'p': 19,\n",
    "                        'z': 20,\n",
    "                        'e': 21,\n",
    "                        'tÌª': 22,\n",
    "                        'u': 23,\n",
    "                        'j': 24,\n",
    "                        'dÌª': 25,\n",
    "                        'o': 26,\n",
    "                        'i': 27,\n",
    "                        'dÍ¡z': 28,\n",
    "                        's': 29,\n",
    "                        'dÍ¡zÌ¤': 30,\n",
    "                        'à¦ƒ': 31,\n",
    "                        'h': 32,\n",
    "                        'à§': 33,\n",
    "                        'É½': 34,\n",
    "                        'Ìƒ': 35,\n",
    "                        'l': 36,\n",
    "                        'Êˆ': 37,\n",
    "                        'É¡': 38,\n",
    "                        'É”': 39,\n",
    "                        ' ': 40}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "\n",
    "        item[\"audio_file\"] = item[\"audio_file\"] + \".wav\"\n",
    "        raw_text = item[\"text\"]\n",
    "\n",
    "        raw_text, token_ids = self.get_text(raw_text)\n",
    "\n",
    "        wav, _ = load_audio(item[\"audio_file\"])\n",
    "        if self.model_args.encoder_sample_rate is not None:\n",
    "            if wav.size(1) % self.model_args.encoder_sample_rate != 0:\n",
    "                wav = wav[:, : -int(wav.size(1) % self.model_args.encoder_sample_rate)]\n",
    "\n",
    "        wav_filename = os.path.basename(item[\"audio_file\"])\n",
    "\n",
    "        # token_ids = ipa_tokens\n",
    "\n",
    "        # after phonemization the text length may change\n",
    "        # this is a shameful ðŸ¤­ hack to prevent longer phonemes\n",
    "        # TODO: find a better fix\n",
    "        if len(token_ids) > self.max_text_len or wav.shape[1] < self.min_audio_len:\n",
    "            self.rescue_item_idx += 1\n",
    "            return self.__getitem__(self.rescue_item_idx)\n",
    "\n",
    "        return {\n",
    "            \"raw_text\": raw_text,\n",
    "            \"token_ids\": token_ids,\n",
    "            \"token_len\": len(token_ids),\n",
    "            \"wav\": wav,\n",
    "            \"wav_file\": wav_filename,\n",
    "            \"speaker_name\": item[\"speaker_name\"],\n",
    "            \"language_name\": item[\"language\"],\n",
    "            \"audio_unique_name\": item[\"audio_unique_name\"],\n",
    "        }\n",
    "    \n",
    "    def get_text(self, text):\n",
    "        text_norm = replace_number_with_text(text)\n",
    "        text_norm = bangla_text_normalize(text_norm)\n",
    "        text_to_phonemes = self.bn_phonemizer.trans_list(text_norm)\n",
    "        ipa_tokens = self.get_ipa_tokens_from_text(text_to_phonemes)\n",
    "        text_norm = torch.IntTensor(ipa_tokens)\n",
    "\n",
    "        return text_norm, ipa_tokens\n",
    "    \n",
    "    def get_ipa_tokens_from_text(self, tokens):\n",
    "        ipa_tokens = []\n",
    "        for tok in tokens:\n",
    "            if tok in self.ipa_dict.keys():\n",
    "                ipa_tokens.append(self.ipa_dict[tok])\n",
    "\n",
    "        return ipa_tokens\n",
    "\n",
    "    @property\n",
    "    def lengths(self):\n",
    "        lens = []\n",
    "        for item in self.samples:\n",
    "            _, wav_file, *_ = _parse_sample(item)\n",
    "            audio_len = os.path.getsize(wav_file) / 16 * 8  # assuming 16bit audio\n",
    "            lens.append(audio_len)\n",
    "        return lens\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Return Shapes:\n",
    "            - tokens: :math:`[B, T]`\n",
    "            - token_lens :math:`[B]`\n",
    "            - token_rel_lens :math:`[B]`\n",
    "            - waveform: :math:`[B, 1, T]`\n",
    "            - waveform_lens: :math:`[B]`\n",
    "            - waveform_rel_lens: :math:`[B]`\n",
    "            - speaker_names: :math:`[B]`\n",
    "            - language_names: :math:`[B]`\n",
    "            - audiofile_paths: :math:`[B]`\n",
    "            - raw_texts: :math:`[B]`\n",
    "            - audio_unique_names: :math:`[B]`\n",
    "        \"\"\"\n",
    "        # convert list of dicts to dict of lists\n",
    "        B = len(batch)\n",
    "        batch = {k: [dic[k] for dic in batch] for k in batch[0]}\n",
    "\n",
    "        _, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([x.size(1) for x in batch[\"wav\"]]), dim=0, descending=True\n",
    "        )\n",
    "\n",
    "        max_text_len = max([len(x) for x in batch[\"token_ids\"]])\n",
    "        token_lens = torch.LongTensor(batch[\"token_len\"])\n",
    "        token_rel_lens = token_lens / token_lens.max()\n",
    "\n",
    "        wav_lens = [w.shape[1] for w in batch[\"wav\"]]\n",
    "        wav_lens = torch.LongTensor(wav_lens)\n",
    "        wav_lens_max = torch.max(wav_lens)\n",
    "        wav_rel_lens = wav_lens / wav_lens_max\n",
    "\n",
    "        token_padded = torch.LongTensor(B, max_text_len)\n",
    "        wav_padded = torch.FloatTensor(B, 1, wav_lens_max)\n",
    "        token_padded = token_padded.zero_() + self.pad_id\n",
    "        wav_padded = wav_padded.zero_() + self.pad_id\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            token_ids = batch[\"token_ids\"][i]\n",
    "            token_padded[i, : batch[\"token_len\"][i]] = torch.LongTensor(token_ids)\n",
    "\n",
    "            wav = batch[\"wav\"][i]\n",
    "            wav_padded[i, :, : wav.size(1)] = torch.FloatTensor(wav)\n",
    "\n",
    "        return {\n",
    "            \"tokens\": token_padded,\n",
    "            \"token_lens\": token_lens,\n",
    "            \"token_rel_lens\": token_rel_lens,\n",
    "            \"waveform\": wav_padded,  # (B x T)\n",
    "            \"waveform_lens\": wav_lens,  # (B)\n",
    "            \"waveform_rel_lens\": wav_rel_lens,\n",
    "            \"speaker_names\": batch[\"speaker_name\"],\n",
    "            \"language_names\": batch[\"language_name\"],\n",
    "            \"audio_files\": batch[\"wav_file\"],\n",
    "            \"raw_text\": batch[\"raw_text\"],\n",
    "            \"audio_unique_names\": batch[\"audio_unique_name\"],\n",
    "        }\n",
    "\n",
    "\n",
    "##############################\n",
    "# MODEL DEFINITION\n",
    "##############################\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VitsArgs(Coqpit):\n",
    "    \"\"\"VITS model arguments.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        num_chars (int):\n",
    "            Number of characters in the vocabulary. Defaults to 100.\n",
    "\n",
    "        out_channels (int):\n",
    "            Number of output channels of the decoder. Defaults to 513.\n",
    "\n",
    "        spec_segment_size (int):\n",
    "            Decoder input segment size. Defaults to 32 `(32 * hoplength = waveform length)`.\n",
    "\n",
    "        hidden_channels (int):\n",
    "            Number of hidden channels of the model. Defaults to 192.\n",
    "\n",
    "        hidden_channels_ffn_text_encoder (int):\n",
    "            Number of hidden channels of the feed-forward layers of the text encoder transformer. Defaults to 256.\n",
    "\n",
    "        num_heads_text_encoder (int):\n",
    "            Number of attention heads of the text encoder transformer. Defaults to 2.\n",
    "\n",
    "        num_layers_text_encoder (int):\n",
    "            Number of transformer layers in the text encoder. Defaults to 6.\n",
    "\n",
    "        kernel_size_text_encoder (int):\n",
    "            Kernel size of the text encoder transformer FFN layers. Defaults to 3.\n",
    "\n",
    "        dropout_p_text_encoder (float):\n",
    "            Dropout rate of the text encoder. Defaults to 0.1.\n",
    "\n",
    "        dropout_p_duration_predictor (float):\n",
    "            Dropout rate of the duration predictor. Defaults to 0.1.\n",
    "\n",
    "        kernel_size_posterior_encoder (int):\n",
    "            Kernel size of the posterior encoder's WaveNet layers. Defaults to 5.\n",
    "\n",
    "        dilatation_posterior_encoder (int):\n",
    "            Dilation rate of the posterior encoder's WaveNet layers. Defaults to 1.\n",
    "\n",
    "        num_layers_posterior_encoder (int):\n",
    "            Number of posterior encoder's WaveNet layers. Defaults to 16.\n",
    "\n",
    "        kernel_size_flow (int):\n",
    "            Kernel size of the Residual Coupling layers of the flow network. Defaults to 5.\n",
    "\n",
    "        dilatation_flow (int):\n",
    "            Dilation rate of the Residual Coupling WaveNet layers of the flow network. Defaults to 1.\n",
    "\n",
    "        num_layers_flow (int):\n",
    "            Number of Residual Coupling WaveNet layers of the flow network. Defaults to 6.\n",
    "\n",
    "        resblock_type_decoder (str):\n",
    "            Type of the residual block in the decoder network. Defaults to \"1\".\n",
    "\n",
    "        resblock_kernel_sizes_decoder (List[int]):\n",
    "            Kernel sizes of the residual blocks in the decoder network. Defaults to `[3, 7, 11]`.\n",
    "\n",
    "        resblock_dilation_sizes_decoder (List[List[int]]):\n",
    "            Dilation sizes of the residual blocks in the decoder network. Defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`.\n",
    "\n",
    "        upsample_rates_decoder (List[int]):\n",
    "            Upsampling rates for each concecutive upsampling layer in the decoder network. The multiply of these\n",
    "            values must be equal to the kop length used for computing spectrograms. Defaults to `[8, 8, 2, 2]`.\n",
    "\n",
    "        upsample_initial_channel_decoder (int):\n",
    "            Number of hidden channels of the first upsampling convolution layer of the decoder network. Defaults to 512.\n",
    "\n",
    "        upsample_kernel_sizes_decoder (List[int]):\n",
    "            Kernel sizes for each upsampling layer of the decoder network. Defaults to `[16, 16, 4, 4]`.\n",
    "\n",
    "        periods_multi_period_discriminator (List[int]):\n",
    "            Periods values for Vits Multi-Period Discriminator. Defaults to `[2, 3, 5, 7, 11]`.\n",
    "\n",
    "        use_sdp (bool):\n",
    "            Use Stochastic Duration Predictor. Defaults to True.\n",
    "\n",
    "        noise_scale (float):\n",
    "            Noise scale used for the sample noise tensor in training. Defaults to 1.0.\n",
    "\n",
    "        inference_noise_scale (float):\n",
    "            Noise scale used for the sample noise tensor in inference. Defaults to 0.667.\n",
    "\n",
    "        length_scale (float):\n",
    "            Scale factor for the predicted duration values. Smaller values result faster speech. Defaults to 1.\n",
    "\n",
    "        noise_scale_dp (float):\n",
    "            Noise scale used by the Stochastic Duration Predictor sample noise in training. Defaults to 1.0.\n",
    "\n",
    "        inference_noise_scale_dp (float):\n",
    "            Noise scale for the Stochastic Duration Predictor in inference. Defaults to 0.8.\n",
    "\n",
    "        max_inference_len (int):\n",
    "            Maximum inference length to limit the memory use. Defaults to None.\n",
    "\n",
    "        init_discriminator (bool):\n",
    "            Initialize the disciminator network if set True. Set False for inference. Defaults to True.\n",
    "\n",
    "        use_spectral_norm_disriminator (bool):\n",
    "            Use spectral normalization over weight norm in the discriminator. Defaults to False.\n",
    "\n",
    "        use_speaker_embedding (bool):\n",
    "            Enable/Disable speaker embedding for multi-speaker models. Defaults to False.\n",
    "\n",
    "        num_speakers (int):\n",
    "            Number of speakers for the speaker embedding layer. Defaults to 0.\n",
    "\n",
    "        speakers_file (str):\n",
    "            Path to the speaker mapping file for the Speaker Manager. Defaults to None.\n",
    "\n",
    "        speaker_embedding_channels (int):\n",
    "            Number of speaker embedding channels. Defaults to 256.\n",
    "\n",
    "        use_d_vector_file (bool):\n",
    "            Enable/Disable the use of d-vectors for multi-speaker training. Defaults to False.\n",
    "\n",
    "        d_vector_file (List[str]):\n",
    "            List of paths to the files including pre-computed speaker embeddings. Defaults to None.\n",
    "\n",
    "        d_vector_dim (int):\n",
    "            Number of d-vector channels. Defaults to 0.\n",
    "\n",
    "        detach_dp_input (bool):\n",
    "            Detach duration predictor's input from the network for stopping the gradients. Defaults to True.\n",
    "\n",
    "        use_language_embedding (bool):\n",
    "            Enable/Disable language embedding for multilingual models. Defaults to False.\n",
    "\n",
    "        embedded_language_dim (int):\n",
    "            Number of language embedding channels. Defaults to 4.\n",
    "\n",
    "        num_languages (int):\n",
    "            Number of languages for the language embedding layer. Defaults to 0.\n",
    "\n",
    "        language_ids_file (str):\n",
    "            Path to the language mapping file for the Language Manager. Defaults to None.\n",
    "\n",
    "        use_speaker_encoder_as_loss (bool):\n",
    "            Enable/Disable Speaker Consistency Loss (SCL). Defaults to False.\n",
    "\n",
    "        speaker_encoder_config_path (str):\n",
    "            Path to the file speaker encoder config file, to use for SCL. Defaults to \"\".\n",
    "\n",
    "        speaker_encoder_model_path (str):\n",
    "            Path to the file speaker encoder checkpoint file, to use for SCL. Defaults to \"\".\n",
    "\n",
    "        condition_dp_on_speaker (bool):\n",
    "            Condition the duration predictor on the speaker embedding. Defaults to True.\n",
    "\n",
    "        freeze_encoder (bool):\n",
    "            Freeze the encoder weigths during training. Defaults to False.\n",
    "\n",
    "        freeze_DP (bool):\n",
    "            Freeze the duration predictor weigths during training. Defaults to False.\n",
    "\n",
    "        freeze_PE (bool):\n",
    "            Freeze the posterior encoder weigths during training. Defaults to False.\n",
    "\n",
    "        freeze_flow_encoder (bool):\n",
    "            Freeze the flow encoder weigths during training. Defaults to False.\n",
    "\n",
    "        freeze_waveform_decoder (bool):\n",
    "            Freeze the waveform decoder weigths during training. Defaults to False.\n",
    "\n",
    "        encoder_sample_rate (int):\n",
    "            If not None this sample rate will be used for training the Posterior Encoder,\n",
    "            flow, text_encoder and duration predictor. The decoder part (vocoder) will be\n",
    "            trained with the `config.audio.sample_rate`. Defaults to None.\n",
    "\n",
    "        interpolate_z (bool):\n",
    "            If `encoder_sample_rate` not None and  this parameter True the nearest interpolation\n",
    "            will be used to upsampling the latent variable z with the sampling rate `encoder_sample_rate`\n",
    "            to the `config.audio.sample_rate`. If it is False you will need to add extra\n",
    "            `upsample_rates_decoder` to match the shape. Defaults to True.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    num_chars: int = 100\n",
    "    out_channels: int = 513\n",
    "    spec_segment_size: int = 32\n",
    "    hidden_channels: int = 192\n",
    "    hidden_channels_ffn_text_encoder: int = 768\n",
    "    num_heads_text_encoder: int = 2\n",
    "    num_layers_text_encoder: int = 6\n",
    "    kernel_size_text_encoder: int = 3\n",
    "    dropout_p_text_encoder: float = 0.1\n",
    "    dropout_p_duration_predictor: float = 0.5\n",
    "    kernel_size_posterior_encoder: int = 5\n",
    "    dilation_rate_posterior_encoder: int = 1\n",
    "    num_layers_posterior_encoder: int = 16\n",
    "    kernel_size_flow: int = 5\n",
    "    dilation_rate_flow: int = 1\n",
    "    num_layers_flow: int = 4\n",
    "    resblock_type_decoder: str = \"1\"\n",
    "    resblock_kernel_sizes_decoder: List[int] = field(default_factory=lambda: [3, 7, 11])\n",
    "    resblock_dilation_sizes_decoder: List[List[int]] = field(default_factory=lambda: [[1, 3, 5], [1, 3, 5], [1, 3, 5]])\n",
    "    upsample_rates_decoder: List[int] = field(default_factory=lambda: [8, 8, 2, 2])\n",
    "    upsample_initial_channel_decoder: int = 512\n",
    "    upsample_kernel_sizes_decoder: List[int] = field(default_factory=lambda: [16, 16, 4, 4])\n",
    "    periods_multi_period_discriminator: List[int] = field(default_factory=lambda: [2, 3, 5, 7, 11])\n",
    "    use_sdp: bool = True\n",
    "    noise_scale: float = 1.0\n",
    "    inference_noise_scale: float = 0.667\n",
    "    length_scale: float = 1\n",
    "    noise_scale_dp: float = 1.0\n",
    "    inference_noise_scale_dp: float = 1.0\n",
    "    max_inference_len: int = None\n",
    "    init_discriminator: bool = True\n",
    "    use_spectral_norm_disriminator: bool = False\n",
    "    use_speaker_embedding: bool = False\n",
    "    num_speakers: int = 0\n",
    "    speakers_file: str = None\n",
    "    d_vector_file: List[str] = None\n",
    "    speaker_embedding_channels: int = 256\n",
    "    use_d_vector_file: bool = False\n",
    "    d_vector_dim: int = 0\n",
    "    detach_dp_input: bool = True\n",
    "    use_language_embedding: bool = False\n",
    "    embedded_language_dim: int = 4\n",
    "    num_languages: int = 0\n",
    "    language_ids_file: str = None\n",
    "    use_speaker_encoder_as_loss: bool = False\n",
    "    speaker_encoder_config_path: str = \"\"\n",
    "    speaker_encoder_model_path: str = \"\"\n",
    "    condition_dp_on_speaker: bool = True\n",
    "    freeze_encoder: bool = False\n",
    "    freeze_DP: bool = False\n",
    "    freeze_PE: bool = False\n",
    "    freeze_flow_decoder: bool = False\n",
    "    freeze_waveform_decoder: bool = False\n",
    "    encoder_sample_rate: int = None\n",
    "    interpolate_z: bool = True\n",
    "    reinit_DP: bool = False\n",
    "    reinit_text_encoder: bool = False\n",
    "\n",
    "\n",
    "class Vits(BaseTTS):\n",
    "    \"\"\"VITS TTS model\n",
    "\n",
    "    Paper::\n",
    "        https://arxiv.org/pdf/2106.06103.pdf\n",
    "\n",
    "    Paper Abstract::\n",
    "        Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel\n",
    "        sampling have been proposed, but their sample quality does not match that of two-stage TTS systems.\n",
    "        In this work, we present a parallel endto-end TTS method that generates more natural sounding audio than\n",
    "        current two-stage models. Our method adopts variational inference augmented with normalizing flows and\n",
    "        an adversarial training process, which improves the expressive power of generative modeling. We also propose a\n",
    "        stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the\n",
    "        uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the\n",
    "        natural one-to-many relationship in which a text input can be spoken in multiple ways\n",
    "        with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS)\n",
    "        on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly\n",
    "        available TTS systems and achieves a MOS comparable to ground truth.\n",
    "\n",
    "    Check :class:`TTS.tts.configs.vits_config.VitsConfig` for class arguments.\n",
    "\n",
    "    Examples:\n",
    "        >>> from TTS.tts.configs.vits_config import VitsConfig\n",
    "        >>> from TTS.tts.models.vits import Vits\n",
    "        >>> config = VitsConfig()\n",
    "        >>> model = Vits(config)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Coqpit,\n",
    "        ap: \"AudioProcessor\" = None,\n",
    "        tokenizer: \"TTSTokenizer\" = None,\n",
    "        speaker_manager: SpeakerManager = None,\n",
    "        language_manager: LanguageManager = None,\n",
    "    ):\n",
    "        super().__init__(config, ap, tokenizer, speaker_manager, language_manager)\n",
    "\n",
    "        self.init_multispeaker(config)\n",
    "        self.init_multilingual(config)\n",
    "        self.init_upsampling()\n",
    "\n",
    "        self.length_scale = self.args.length_scale\n",
    "        self.noise_scale = self.args.noise_scale\n",
    "        self.inference_noise_scale = self.args.inference_noise_scale\n",
    "        self.inference_noise_scale_dp = self.args.inference_noise_scale_dp\n",
    "        self.noise_scale_dp = self.args.noise_scale_dp\n",
    "        self.max_inference_len = self.args.max_inference_len\n",
    "        self.spec_segment_size = self.args.spec_segment_size\n",
    "\n",
    "        self.text_encoder = TextEncoder(\n",
    "            self.args.num_chars,\n",
    "            self.args.hidden_channels,\n",
    "            self.args.hidden_channels,\n",
    "            self.args.hidden_channels_ffn_text_encoder,\n",
    "            self.args.num_heads_text_encoder,\n",
    "            self.args.num_layers_text_encoder,\n",
    "            self.args.kernel_size_text_encoder,\n",
    "            self.args.dropout_p_text_encoder,\n",
    "            language_emb_dim=self.embedded_language_dim,\n",
    "        )\n",
    "\n",
    "        self.posterior_encoder = PosteriorEncoder(\n",
    "            self.args.out_channels,\n",
    "            self.args.hidden_channels,\n",
    "            self.args.hidden_channels,\n",
    "            kernel_size=self.args.kernel_size_posterior_encoder,\n",
    "            dilation_rate=self.args.dilation_rate_posterior_encoder,\n",
    "            num_layers=self.args.num_layers_posterior_encoder,\n",
    "            cond_channels=self.embedded_speaker_dim,\n",
    "        )\n",
    "\n",
    "        self.flow = ResidualCouplingBlocks(\n",
    "            self.args.hidden_channels,\n",
    "            self.args.hidden_channels,\n",
    "            kernel_size=self.args.kernel_size_flow,\n",
    "            dilation_rate=self.args.dilation_rate_flow,\n",
    "            num_layers=self.args.num_layers_flow,\n",
    "            cond_channels=self.embedded_speaker_dim,\n",
    "        )\n",
    "\n",
    "        if self.args.use_sdp:\n",
    "            self.duration_predictor = StochasticDurationPredictor(\n",
    "                self.args.hidden_channels,\n",
    "                192,\n",
    "                3,\n",
    "                self.args.dropout_p_duration_predictor,\n",
    "                4,\n",
    "                cond_channels=self.embedded_speaker_dim if self.args.condition_dp_on_speaker else 0,\n",
    "                language_emb_dim=self.embedded_language_dim,\n",
    "            )\n",
    "        else:\n",
    "            self.duration_predictor = DurationPredictor(\n",
    "                self.args.hidden_channels,\n",
    "                256,\n",
    "                3,\n",
    "                self.args.dropout_p_duration_predictor,\n",
    "                cond_channels=self.embedded_speaker_dim,\n",
    "                language_emb_dim=self.embedded_language_dim,\n",
    "            )\n",
    "\n",
    "        self.waveform_decoder = HifiganGenerator(\n",
    "            self.args.hidden_channels,\n",
    "            1,\n",
    "            self.args.resblock_type_decoder,\n",
    "            self.args.resblock_dilation_sizes_decoder,\n",
    "            self.args.resblock_kernel_sizes_decoder,\n",
    "            self.args.upsample_kernel_sizes_decoder,\n",
    "            self.args.upsample_initial_channel_decoder,\n",
    "            self.args.upsample_rates_decoder,\n",
    "            inference_padding=0,\n",
    "            cond_channels=self.embedded_speaker_dim,\n",
    "            conv_pre_weight_norm=False,\n",
    "            conv_post_weight_norm=False,\n",
    "            conv_post_bias=False,\n",
    "        )\n",
    "\n",
    "        if self.args.init_discriminator:\n",
    "            self.disc = VitsDiscriminator(\n",
    "                periods=self.args.periods_multi_period_discriminator,\n",
    "                use_spectral_norm=self.args.use_spectral_norm_disriminator,\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def init_multispeaker(self, config: Coqpit):\n",
    "        \"\"\"Initialize multi-speaker modules of a model. A model can be trained either with a speaker embedding layer\n",
    "        or with external `d_vectors` computed from a speaker encoder model.\n",
    "\n",
    "        You must provide a `speaker_manager` at initialization to set up the multi-speaker modules.\n",
    "\n",
    "        Args:\n",
    "            config (Coqpit): Model configuration.\n",
    "            data (List, optional): Dataset items to infer number of speakers. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.embedded_speaker_dim = 0\n",
    "        self.num_speakers = self.args.num_speakers\n",
    "        self.audio_transform = None\n",
    "\n",
    "        if self.speaker_manager:\n",
    "            self.num_speakers = self.speaker_manager.num_speakers\n",
    "\n",
    "        if self.args.use_speaker_embedding:\n",
    "            self._init_speaker_embedding()\n",
    "\n",
    "        if self.args.use_d_vector_file:\n",
    "            self._init_d_vector()\n",
    "\n",
    "        # TODO: make this a function\n",
    "        if self.args.use_speaker_encoder_as_loss:\n",
    "            if self.speaker_manager.encoder is None and (\n",
    "                not self.args.speaker_encoder_model_path or not self.args.speaker_encoder_config_path\n",
    "            ):\n",
    "                raise RuntimeError(\n",
    "                    \" [!] To use the speaker consistency loss (SCL) you need to specify speaker_encoder_model_path and speaker_encoder_config_path !!\"\n",
    "                )\n",
    "\n",
    "            self.speaker_manager.encoder.eval()\n",
    "            print(\" > External Speaker Encoder Loaded !!\")\n",
    "\n",
    "            if (\n",
    "                hasattr(self.speaker_manager.encoder, \"audio_config\")\n",
    "                and self.config.audio.sample_rate != self.speaker_manager.encoder.audio_config[\"sample_rate\"]\n",
    "            ):\n",
    "                self.audio_transform = torchaudio.transforms.Resample(\n",
    "                    orig_freq=self.config.audio.sample_rate,\n",
    "                    new_freq=self.speaker_manager.encoder.audio_config[\"sample_rate\"],\n",
    "                )\n",
    "\n",
    "    def _init_speaker_embedding(self):\n",
    "        # pylint: disable=attribute-defined-outside-init\n",
    "        if self.num_speakers > 0:\n",
    "            print(\" > initialization of speaker-embedding layers.\")\n",
    "            self.embedded_speaker_dim = self.args.speaker_embedding_channels\n",
    "            self.emb_g = nn.Embedding(self.num_speakers, self.embedded_speaker_dim)\n",
    "\n",
    "    def _init_d_vector(self):\n",
    "        # pylint: disable=attribute-defined-outside-init\n",
    "        if hasattr(self, \"emb_g\"):\n",
    "            raise ValueError(\"[!] Speaker embedding layer already initialized before d_vector settings.\")\n",
    "        self.embedded_speaker_dim = self.args.d_vector_dim\n",
    "\n",
    "    def init_multilingual(self, config: Coqpit):\n",
    "        \"\"\"Initialize multilingual modules of a model.\n",
    "\n",
    "        Args:\n",
    "            config (Coqpit): Model configuration.\n",
    "        \"\"\"\n",
    "        if self.args.language_ids_file is not None:\n",
    "            self.language_manager = LanguageManager(language_ids_file_path=config.language_ids_file)\n",
    "\n",
    "        if self.args.use_language_embedding and self.language_manager:\n",
    "            print(\" > initialization of language-embedding layers.\")\n",
    "            self.num_languages = self.language_manager.num_languages\n",
    "            self.embedded_language_dim = self.args.embedded_language_dim\n",
    "            self.emb_l = nn.Embedding(self.num_languages, self.embedded_language_dim)\n",
    "            torch.nn.init.xavier_uniform_(self.emb_l.weight)\n",
    "        else:\n",
    "            self.embedded_language_dim = 0\n",
    "\n",
    "    def init_upsampling(self):\n",
    "        \"\"\"\n",
    "        Initialize upsampling modules of a model.\n",
    "        \"\"\"\n",
    "        if self.args.encoder_sample_rate:\n",
    "            self.interpolate_factor = self.config.audio[\"sample_rate\"] / self.args.encoder_sample_rate\n",
    "            self.audio_resampler = torchaudio.transforms.Resample(\n",
    "                orig_freq=self.config.audio[\"sample_rate\"], new_freq=self.args.encoder_sample_rate\n",
    "            )  # pylint: disable=W0201\n",
    "\n",
    "    def on_epoch_start(self, trainer):  # pylint: disable=W0613\n",
    "        \"\"\"Freeze layers at the beginning of an epoch\"\"\"\n",
    "        self._freeze_layers()\n",
    "        # set the device of speaker encoder\n",
    "        if self.args.use_speaker_encoder_as_loss:\n",
    "            self.speaker_manager.encoder = self.speaker_manager.encoder.to(self.device)\n",
    "\n",
    "    def on_init_end(self, trainer):  # pylint: disable=W0613\n",
    "        \"\"\"Reinit layes if needed\"\"\"\n",
    "        if self.args.reinit_DP:\n",
    "            before_dict = get_module_weights_sum(self.duration_predictor)\n",
    "            # Applies weights_reset recursively to every submodule of the duration predictor\n",
    "            self.duration_predictor.apply(fn=weights_reset)\n",
    "            after_dict = get_module_weights_sum(self.duration_predictor)\n",
    "            for key, value in after_dict.items():\n",
    "                if value == before_dict[key]:\n",
    "                    raise RuntimeError(\" [!] The weights of Duration Predictor was not reinit check it !\")\n",
    "            print(\" > Duration Predictor was reinit.\")\n",
    "\n",
    "        if self.args.reinit_text_encoder:\n",
    "            before_dict = get_module_weights_sum(self.text_encoder)\n",
    "            # Applies weights_reset recursively to every submodule of the duration predictor\n",
    "            self.text_encoder.apply(fn=weights_reset)\n",
    "            after_dict = get_module_weights_sum(self.text_encoder)\n",
    "            for key, value in after_dict.items():\n",
    "                if value == before_dict[key]:\n",
    "                    raise RuntimeError(\" [!] The weights of Text Encoder was not reinit check it !\")\n",
    "            print(\" > Text Encoder was reinit.\")\n",
    "\n",
    "    def get_aux_input(self, aux_input: Dict):\n",
    "        sid, g, lid, _ = self._set_cond_input(aux_input)\n",
    "        return {\"speaker_ids\": sid, \"style_wav\": None, \"d_vectors\": g, \"language_ids\": lid}\n",
    "\n",
    "    def _freeze_layers(self):\n",
    "        if self.args.freeze_encoder:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            if hasattr(self, \"emb_l\"):\n",
    "                for param in self.emb_l.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        if self.args.freeze_PE:\n",
    "            for param in self.posterior_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.args.freeze_DP:\n",
    "            for param in self.duration_predictor.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.args.freeze_flow_decoder:\n",
    "            for param in self.flow.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.args.freeze_waveform_decoder:\n",
    "            for param in self.waveform_decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    @staticmethod\n",
    "    def _set_cond_input(aux_input: Dict):\n",
    "        \"\"\"Set the speaker conditioning input based on the multi-speaker mode.\"\"\"\n",
    "        sid, g, lid, durations = None, None, None, None\n",
    "        if \"speaker_ids\" in aux_input and aux_input[\"speaker_ids\"] is not None:\n",
    "            sid = aux_input[\"speaker_ids\"]\n",
    "            if sid.ndim == 0:\n",
    "                sid = sid.unsqueeze_(0)\n",
    "        if \"d_vectors\" in aux_input and aux_input[\"d_vectors\"] is not None:\n",
    "            g = F.normalize(aux_input[\"d_vectors\"]).unsqueeze(-1)\n",
    "            if g.ndim == 2:\n",
    "                g = g.unsqueeze_(0)\n",
    "\n",
    "        if \"language_ids\" in aux_input and aux_input[\"language_ids\"] is not None:\n",
    "            lid = aux_input[\"language_ids\"]\n",
    "            if lid.ndim == 0:\n",
    "                lid = lid.unsqueeze_(0)\n",
    "\n",
    "        if \"durations\" in aux_input and aux_input[\"durations\"] is not None:\n",
    "            durations = aux_input[\"durations\"]\n",
    "\n",
    "        return sid, g, lid, durations\n",
    "\n",
    "    def _set_speaker_input(self, aux_input: Dict):\n",
    "        d_vectors = aux_input.get(\"d_vectors\", None)\n",
    "        speaker_ids = aux_input.get(\"speaker_ids\", None)\n",
    "\n",
    "        if d_vectors is not None and speaker_ids is not None:\n",
    "            raise ValueError(\"[!] Cannot use d-vectors and speaker-ids together.\")\n",
    "\n",
    "        if speaker_ids is not None and not hasattr(self, \"emb_g\"):\n",
    "            raise ValueError(\"[!] Cannot use speaker-ids without enabling speaker embedding.\")\n",
    "\n",
    "        g = speaker_ids if speaker_ids is not None else d_vectors\n",
    "        return g\n",
    "\n",
    "    def forward_mas(self, outputs, z_p, m_p, logs_p, x, x_mask, y_mask, g, lang_emb):\n",
    "        # find the alignment path\n",
    "        attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(y_mask, 2)\n",
    "        with torch.no_grad():\n",
    "            o_scale = torch.exp(-2 * logs_p)\n",
    "            logp1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1]).unsqueeze(-1)  # [b, t, 1]\n",
    "            logp2 = torch.einsum(\"klm, kln -> kmn\", [o_scale, -0.5 * (z_p**2)])\n",
    "            logp3 = torch.einsum(\"klm, kln -> kmn\", [m_p * o_scale, z_p])\n",
    "            logp4 = torch.sum(-0.5 * (m_p**2) * o_scale, [1]).unsqueeze(-1)  # [b, t, 1]\n",
    "            logp = logp2 + logp3 + logp1 + logp4\n",
    "            attn = maximum_path(logp, attn_mask.squeeze(1)).unsqueeze(1).detach()  # [b, 1, t, t']\n",
    "\n",
    "        # duration predictor\n",
    "        attn_durations = attn.sum(3)\n",
    "        if self.args.use_sdp:\n",
    "            loss_duration = self.duration_predictor(\n",
    "                x.detach() if self.args.detach_dp_input else x,\n",
    "                x_mask,\n",
    "                attn_durations,\n",
    "                g=g.detach() if self.args.detach_dp_input and g is not None else g,\n",
    "                lang_emb=lang_emb.detach() if self.args.detach_dp_input and lang_emb is not None else lang_emb,\n",
    "            )\n",
    "            loss_duration = loss_duration / torch.sum(x_mask)\n",
    "        else:\n",
    "            attn_log_durations = torch.log(attn_durations + 1e-6) * x_mask\n",
    "            log_durations = self.duration_predictor(\n",
    "                x.detach() if self.args.detach_dp_input else x,\n",
    "                x_mask,\n",
    "                g=g.detach() if self.args.detach_dp_input and g is not None else g,\n",
    "                lang_emb=lang_emb.detach() if self.args.detach_dp_input and lang_emb is not None else lang_emb,\n",
    "            )\n",
    "            loss_duration = torch.sum((log_durations - attn_log_durations) ** 2, [1, 2]) / torch.sum(x_mask)\n",
    "        outputs[\"loss_duration\"] = loss_duration\n",
    "        return outputs, attn\n",
    "\n",
    "    def upsampling_z(self, z, slice_ids=None, y_lengths=None, y_mask=None):\n",
    "        spec_segment_size = self.spec_segment_size\n",
    "        if self.args.encoder_sample_rate:\n",
    "            # recompute the slices and spec_segment_size if needed\n",
    "            slice_ids = slice_ids * int(self.interpolate_factor) if slice_ids is not None else slice_ids\n",
    "            spec_segment_size = spec_segment_size * int(self.interpolate_factor)\n",
    "            # interpolate z if needed\n",
    "            if self.args.interpolate_z:\n",
    "                z = torch.nn.functional.interpolate(z, scale_factor=[self.interpolate_factor], mode=\"linear\").squeeze(0)\n",
    "                # recompute the mask if needed\n",
    "                if y_lengths is not None and y_mask is not None:\n",
    "                    y_mask = (\n",
    "                        sequence_mask(y_lengths * self.interpolate_factor, None).to(y_mask.dtype).unsqueeze(1)\n",
    "                    )  # [B, 1, T_dec_resampled]\n",
    "\n",
    "        return z, spec_segment_size, slice_ids, y_mask\n",
    "\n",
    "    def forward(  # pylint: disable=dangerous-default-value\n",
    "        self,\n",
    "        x: torch.tensor,\n",
    "        x_lengths: torch.tensor,\n",
    "        y: torch.tensor,\n",
    "        y_lengths: torch.tensor,\n",
    "        waveform: torch.tensor,\n",
    "        aux_input={\"d_vectors\": None, \"speaker_ids\": None, \"language_ids\": None},\n",
    "    ) -> Dict:\n",
    "        \"\"\"Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): Batch of input character sequence IDs.\n",
    "            x_lengths (torch.tensor): Batch of input character sequence lengths.\n",
    "            y (torch.tensor): Batch of input spectrograms.\n",
    "            y_lengths (torch.tensor): Batch of input spectrogram lengths.\n",
    "            waveform (torch.tensor): Batch of ground truth waveforms per sample.\n",
    "            aux_input (dict, optional): Auxiliary inputs for multi-speaker and multi-lingual training.\n",
    "                Defaults to {\"d_vectors\": None, \"speaker_ids\": None, \"language_ids\": None}.\n",
    "\n",
    "        Returns:\n",
    "            Dict: model outputs keyed by the output name.\n",
    "\n",
    "        Shapes:\n",
    "            - x: :math:`[B, T_seq]`\n",
    "            - x_lengths: :math:`[B]`\n",
    "            - y: :math:`[B, C, T_spec]`\n",
    "            - y_lengths: :math:`[B]`\n",
    "            - waveform: :math:`[B, 1, T_wav]`\n",
    "            - d_vectors: :math:`[B, C, 1]`\n",
    "            - speaker_ids: :math:`[B]`\n",
    "            - language_ids: :math:`[B]`\n",
    "\n",
    "        Return Shapes:\n",
    "            - model_outputs: :math:`[B, 1, T_wav]`\n",
    "            - alignments: :math:`[B, T_seq, T_dec]`\n",
    "            - z: :math:`[B, C, T_dec]`\n",
    "            - z_p: :math:`[B, C, T_dec]`\n",
    "            - m_p: :math:`[B, C, T_dec]`\n",
    "            - logs_p: :math:`[B, C, T_dec]`\n",
    "            - m_q: :math:`[B, C, T_dec]`\n",
    "            - logs_q: :math:`[B, C, T_dec]`\n",
    "            - waveform_seg: :math:`[B, 1, spec_seg_size * hop_length]`\n",
    "            - gt_spk_emb: :math:`[B, 1, speaker_encoder.proj_dim]`\n",
    "            - syn_spk_emb: :math:`[B, 1, speaker_encoder.proj_dim]`\n",
    "        \"\"\"\n",
    "        outputs = {}\n",
    "        sid, g, lid, _ = self._set_cond_input(aux_input)\n",
    "        # speaker embedding\n",
    "        if self.args.use_speaker_embedding and sid is not None:\n",
    "            g = self.emb_g(sid).unsqueeze(-1)  # [b, h, 1]\n",
    "\n",
    "        # language embedding\n",
    "        lang_emb = None\n",
    "        if self.args.use_language_embedding and lid is not None:\n",
    "            lang_emb = self.emb_l(lid).unsqueeze(-1)\n",
    "\n",
    "        x, m_p, logs_p, x_mask = self.text_encoder(x, x_lengths, lang_emb=lang_emb)\n",
    "\n",
    "        # posterior encoder\n",
    "        z, m_q, logs_q, y_mask = self.posterior_encoder(y, y_lengths, g=g)\n",
    "\n",
    "        # flow layers\n",
    "        z_p = self.flow(z, y_mask, g=g)\n",
    "\n",
    "        # duration predictor\n",
    "        outputs, attn = self.forward_mas(outputs, z_p, m_p, logs_p, x, x_mask, y_mask, g=g, lang_emb=lang_emb)\n",
    "\n",
    "        # expand prior\n",
    "        m_p = torch.einsum(\"klmn, kjm -> kjn\", [attn, m_p])\n",
    "        logs_p = torch.einsum(\"klmn, kjm -> kjn\", [attn, logs_p])\n",
    "\n",
    "        # select a random feature segment for the waveform decoder\n",
    "        z_slice, slice_ids = rand_segments(z, y_lengths, self.spec_segment_size, let_short_samples=True, pad_short=True)\n",
    "\n",
    "        # interpolate z if needed\n",
    "        z_slice, spec_segment_size, slice_ids, _ = self.upsampling_z(z_slice, slice_ids=slice_ids)\n",
    "\n",
    "        o = self.waveform_decoder(z_slice, g=g)\n",
    "\n",
    "        wav_seg = segment(\n",
    "            waveform,\n",
    "            slice_ids * self.config.audio.hop_length,\n",
    "            spec_segment_size * self.config.audio.hop_length,\n",
    "            pad_short=True,\n",
    "        )\n",
    "\n",
    "        if self.args.use_speaker_encoder_as_loss and self.speaker_manager.encoder is not None:\n",
    "            # concate generated and GT waveforms\n",
    "            wavs_batch = torch.cat((wav_seg, o), dim=0)\n",
    "\n",
    "            # resample audio to speaker encoder sample_rate\n",
    "            # pylint: disable=W0105\n",
    "            if self.audio_transform is not None:\n",
    "                wavs_batch = self.audio_transform(wavs_batch)\n",
    "\n",
    "            pred_embs = self.speaker_manager.encoder.forward(wavs_batch, l2_norm=True)\n",
    "\n",
    "            # split generated and GT speaker embeddings\n",
    "            gt_spk_emb, syn_spk_emb = torch.chunk(pred_embs, 2, dim=0)\n",
    "        else:\n",
    "            gt_spk_emb, syn_spk_emb = None, None\n",
    "\n",
    "        outputs.update(\n",
    "            {\n",
    "                \"model_outputs\": o,\n",
    "                \"alignments\": attn.squeeze(1),\n",
    "                \"m_p\": m_p,\n",
    "                \"logs_p\": logs_p,\n",
    "                \"z\": z,\n",
    "                \"z_p\": z_p,\n",
    "                \"m_q\": m_q,\n",
    "                \"logs_q\": logs_q,\n",
    "                \"waveform_seg\": wav_seg,\n",
    "                \"gt_spk_emb\": gt_spk_emb,\n",
    "                \"syn_spk_emb\": syn_spk_emb,\n",
    "                \"slice_ids\": slice_ids,\n",
    "            }\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _set_x_lengths(x, aux_input):\n",
    "        if \"x_lengths\" in aux_input and aux_input[\"x_lengths\"] is not None:\n",
    "            return aux_input[\"x_lengths\"]\n",
    "        return torch.tensor(x.shape[1:2]).to(x.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(\n",
    "        self,\n",
    "        x,\n",
    "        aux_input={\"x_lengths\": None, \"d_vectors\": None, \"speaker_ids\": None, \"language_ids\": None, \"durations\": None},\n",
    "    ):  # pylint: disable=dangerous-default-value\n",
    "        \"\"\"\n",
    "        Note:\n",
    "            To run in batch mode, provide `x_lengths` else model assumes that the batch size is 1.\n",
    "\n",
    "        Shapes:\n",
    "            - x: :math:`[B, T_seq]`\n",
    "            - x_lengths: :math:`[B]`\n",
    "            - d_vectors: :math:`[B, C]`\n",
    "            - speaker_ids: :math:`[B]`\n",
    "\n",
    "        Return Shapes:\n",
    "            - model_outputs: :math:`[B, 1, T_wav]`\n",
    "            - alignments: :math:`[B, T_seq, T_dec]`\n",
    "            - z: :math:`[B, C, T_dec]`\n",
    "            - z_p: :math:`[B, C, T_dec]`\n",
    "            - m_p: :math:`[B, C, T_dec]`\n",
    "            - logs_p: :math:`[B, C, T_dec]`\n",
    "        \"\"\"\n",
    "        sid, g, lid, durations = self._set_cond_input(aux_input)\n",
    "        x_lengths = self._set_x_lengths(x, aux_input)\n",
    "\n",
    "        # speaker embedding\n",
    "        if self.args.use_speaker_embedding and sid is not None:\n",
    "            g = self.emb_g(sid).unsqueeze(-1)\n",
    "\n",
    "        # language embedding\n",
    "        lang_emb = None\n",
    "        if self.args.use_language_embedding and lid is not None:\n",
    "            lang_emb = self.emb_l(lid).unsqueeze(-1)\n",
    "\n",
    "        x, m_p, logs_p, x_mask = self.text_encoder(x, x_lengths, lang_emb=lang_emb)\n",
    "\n",
    "        if durations is None:\n",
    "            if self.args.use_sdp:\n",
    "                logw = self.duration_predictor(\n",
    "                    x,\n",
    "                    x_mask,\n",
    "                    g=g if self.args.condition_dp_on_speaker else None,\n",
    "                    reverse=True,\n",
    "                    noise_scale=self.inference_noise_scale_dp,\n",
    "                    lang_emb=lang_emb,\n",
    "                )\n",
    "            else:\n",
    "                logw = self.duration_predictor(\n",
    "                    x, x_mask, g=g if self.args.condition_dp_on_speaker else None, lang_emb=lang_emb\n",
    "                )\n",
    "            w = torch.exp(logw) * x_mask * self.length_scale\n",
    "        else:\n",
    "            assert durations.shape[-1] == x.shape[-1]\n",
    "            w = durations.unsqueeze(0)\n",
    "\n",
    "        w_ceil = torch.ceil(w)\n",
    "        y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n",
    "        y_mask = sequence_mask(y_lengths, None).to(x_mask.dtype).unsqueeze(1)  # [B, 1, T_dec]\n",
    "\n",
    "        attn_mask = x_mask * y_mask.transpose(1, 2)  # [B, 1, T_enc] * [B, T_dec, 1]\n",
    "        attn = generate_path(w_ceil.squeeze(1), attn_mask.squeeze(1).transpose(1, 2))\n",
    "\n",
    "        m_p = torch.matmul(attn.transpose(1, 2), m_p.transpose(1, 2)).transpose(1, 2)\n",
    "        logs_p = torch.matmul(attn.transpose(1, 2), logs_p.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * self.inference_noise_scale\n",
    "        z = self.flow(z_p, y_mask, g=g, reverse=True)\n",
    "\n",
    "        # upsampling if needed\n",
    "        z, _, _, y_mask = self.upsampling_z(z, y_lengths=y_lengths, y_mask=y_mask)\n",
    "\n",
    "        o = self.waveform_decoder((z * y_mask)[:, :, : self.max_inference_len], g=g)\n",
    "\n",
    "        outputs = {\n",
    "            \"model_outputs\": o,\n",
    "            \"alignments\": attn.squeeze(1),\n",
    "            \"durations\": w_ceil,\n",
    "            \"z\": z,\n",
    "            \"z_p\": z_p,\n",
    "            \"m_p\": m_p,\n",
    "            \"logs_p\": logs_p,\n",
    "            \"y_mask\": y_mask,\n",
    "        }\n",
    "        return outputs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference_voice_conversion(\n",
    "        self, reference_wav, speaker_id=None, d_vector=None, reference_speaker_id=None, reference_d_vector=None\n",
    "    ):\n",
    "        \"\"\"Inference for voice conversion\n",
    "\n",
    "        Args:\n",
    "            reference_wav (Tensor): Reference wavform. Tensor of shape [B, T]\n",
    "            speaker_id (Tensor): speaker_id of the target speaker. Tensor of shape [B]\n",
    "            d_vector (Tensor): d_vector embedding of target speaker. Tensor of shape `[B, C]`\n",
    "            reference_speaker_id (Tensor): speaker_id of the reference_wav speaker. Tensor of shape [B]\n",
    "            reference_d_vector (Tensor): d_vector embedding of the reference_wav speaker. Tensor of shape `[B, C]`\n",
    "        \"\"\"\n",
    "        # compute spectrograms\n",
    "        y = wav_to_spec(\n",
    "            reference_wav,\n",
    "            self.config.audio.fft_size,\n",
    "            self.config.audio.hop_length,\n",
    "            self.config.audio.win_length,\n",
    "            center=False,\n",
    "        )\n",
    "        y_lengths = torch.tensor([y.size(-1)]).to(y.device)\n",
    "        speaker_cond_src = reference_speaker_id if reference_speaker_id is not None else reference_d_vector\n",
    "        speaker_cond_tgt = speaker_id if speaker_id is not None else d_vector\n",
    "        wav, _, _ = self.voice_conversion(y, y_lengths, speaker_cond_src, speaker_cond_tgt)\n",
    "        return wav\n",
    "\n",
    "    def voice_conversion(self, y, y_lengths, speaker_cond_src, speaker_cond_tgt):\n",
    "        \"\"\"Forward pass for voice conversion\n",
    "\n",
    "        TODO: create an end-point for voice conversion\n",
    "\n",
    "        Args:\n",
    "            y (Tensor): Reference spectrograms. Tensor of shape [B, T, C]\n",
    "            y_lengths (Tensor): Length of each reference spectrogram. Tensor of shape [B]\n",
    "            speaker_cond_src (Tensor): Reference speaker ID. Tensor of shape [B,]\n",
    "            speaker_cond_tgt (Tensor): Target speaker ID. Tensor of shape [B,]\n",
    "        \"\"\"\n",
    "        assert self.num_speakers > 0, \"num_speakers have to be larger than 0.\"\n",
    "        # speaker embedding\n",
    "        if self.args.use_speaker_embedding and not self.args.use_d_vector_file:\n",
    "            g_src = self.emb_g(torch.from_numpy((np.array(speaker_cond_src))).unsqueeze(0)).unsqueeze(-1)\n",
    "            g_tgt = self.emb_g(torch.from_numpy((np.array(speaker_cond_tgt))).unsqueeze(0)).unsqueeze(-1)\n",
    "        elif not self.args.use_speaker_embedding and self.args.use_d_vector_file:\n",
    "            g_src = F.normalize(speaker_cond_src).unsqueeze(-1)\n",
    "            g_tgt = F.normalize(speaker_cond_tgt).unsqueeze(-1)\n",
    "        else:\n",
    "            raise RuntimeError(\" [!] Voice conversion is only supported on multi-speaker models.\")\n",
    "\n",
    "        z, _, _, y_mask = self.posterior_encoder(y, y_lengths, g=g_src)\n",
    "        z_p = self.flow(z, y_mask, g=g_src)\n",
    "        z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n",
    "        o_hat = self.waveform_decoder(z_hat * y_mask, g=g_tgt)\n",
    "        return o_hat, y_mask, (z, z_p, z_hat)\n",
    "\n",
    "    def train_step(self, batch: dict, criterion: nn.Module, optimizer_idx: int) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"Perform a single training step. Run the model forward pass and compute losses.\n",
    "\n",
    "        Args:\n",
    "            batch (Dict): Input tensors.\n",
    "            criterion (nn.Module): Loss layer designed for the model.\n",
    "            optimizer_idx (int): Index of optimizer to use. 0 for the generator and 1 for the discriminator networks.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict, Dict]: Model ouputs and computed losses.\n",
    "        \"\"\"\n",
    "\n",
    "        spec_lens = batch[\"spec_lens\"]\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "            tokens = batch[\"tokens\"]\n",
    "            token_lenghts = batch[\"token_lens\"]\n",
    "            spec = batch[\"spec\"]\n",
    "\n",
    "            d_vectors = batch[\"d_vectors\"]\n",
    "            speaker_ids = batch[\"speaker_ids\"]\n",
    "            language_ids = batch[\"language_ids\"]\n",
    "            waveform = batch[\"waveform\"]\n",
    "\n",
    "            # generator pass\n",
    "            outputs = self.forward(\n",
    "                tokens,\n",
    "                token_lenghts,\n",
    "                spec,\n",
    "                spec_lens,\n",
    "                waveform,\n",
    "                aux_input={\"d_vectors\": d_vectors, \"speaker_ids\": speaker_ids, \"language_ids\": language_ids},\n",
    "            )\n",
    "\n",
    "            # cache tensors for the generator pass\n",
    "            self.model_outputs_cache = outputs  # pylint: disable=attribute-defined-outside-init\n",
    "\n",
    "            # compute scores and features\n",
    "            scores_disc_fake, _, scores_disc_real, _ = self.disc(\n",
    "                outputs[\"model_outputs\"].detach(), outputs[\"waveform_seg\"]\n",
    "            )\n",
    "\n",
    "            # compute loss\n",
    "            with autocast(enabled=False):  # use float32 for the criterion\n",
    "                loss_dict = criterion[optimizer_idx](\n",
    "                    scores_disc_real,\n",
    "                    scores_disc_fake,\n",
    "                )\n",
    "            return outputs, loss_dict\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            mel = batch[\"mel\"]\n",
    "\n",
    "            # compute melspec segment\n",
    "            with autocast(enabled=False):\n",
    "                if self.args.encoder_sample_rate:\n",
    "                    spec_segment_size = self.spec_segment_size * int(self.interpolate_factor)\n",
    "                else:\n",
    "                    spec_segment_size = self.spec_segment_size\n",
    "\n",
    "                mel_slice = segment(\n",
    "                    mel.float(), self.model_outputs_cache[\"slice_ids\"], spec_segment_size, pad_short=True\n",
    "                )\n",
    "                mel_slice_hat = wav_to_mel(\n",
    "                    y=self.model_outputs_cache[\"model_outputs\"].float(),\n",
    "                    n_fft=self.config.audio.fft_size,\n",
    "                    sample_rate=self.config.audio.sample_rate,\n",
    "                    num_mels=self.config.audio.num_mels,\n",
    "                    hop_length=self.config.audio.hop_length,\n",
    "                    win_length=self.config.audio.win_length,\n",
    "                    fmin=self.config.audio.mel_fmin,\n",
    "                    fmax=self.config.audio.mel_fmax,\n",
    "                    center=False,\n",
    "                )\n",
    "\n",
    "            # compute discriminator scores and features\n",
    "            scores_disc_fake, feats_disc_fake, _, feats_disc_real = self.disc(\n",
    "                self.model_outputs_cache[\"model_outputs\"], self.model_outputs_cache[\"waveform_seg\"]\n",
    "            )\n",
    "\n",
    "            # compute losses\n",
    "            with autocast(enabled=False):  # use float32 for the criterion\n",
    "                loss_dict = criterion[optimizer_idx](\n",
    "                    mel_slice_hat=mel_slice.float(),\n",
    "                    mel_slice=mel_slice_hat.float(),\n",
    "                    z_p=self.model_outputs_cache[\"z_p\"].float(),\n",
    "                    logs_q=self.model_outputs_cache[\"logs_q\"].float(),\n",
    "                    m_p=self.model_outputs_cache[\"m_p\"].float(),\n",
    "                    logs_p=self.model_outputs_cache[\"logs_p\"].float(),\n",
    "                    z_len=spec_lens,\n",
    "                    scores_disc_fake=scores_disc_fake,\n",
    "                    feats_disc_fake=feats_disc_fake,\n",
    "                    feats_disc_real=feats_disc_real,\n",
    "                    loss_duration=self.model_outputs_cache[\"loss_duration\"],\n",
    "                    use_speaker_encoder_as_loss=self.args.use_speaker_encoder_as_loss,\n",
    "                    gt_spk_emb=self.model_outputs_cache[\"gt_spk_emb\"],\n",
    "                    syn_spk_emb=self.model_outputs_cache[\"syn_spk_emb\"],\n",
    "                )\n",
    "\n",
    "            return self.model_outputs_cache, loss_dict\n",
    "\n",
    "        raise ValueError(\" [!] Unexpected `optimizer_idx`.\")\n",
    "\n",
    "    def _log(self, ap, batch, outputs, name_prefix=\"train\"):  # pylint: disable=unused-argument,no-self-use\n",
    "        y_hat = outputs[1][\"model_outputs\"]\n",
    "        y = outputs[1][\"waveform_seg\"]\n",
    "        figures = plot_results(y_hat, y, ap, name_prefix)\n",
    "        sample_voice = y_hat[0].squeeze(0).detach().cpu().numpy()\n",
    "        audios = {f\"{name_prefix}/audio\": sample_voice}\n",
    "\n",
    "        alignments = outputs[1][\"alignments\"]\n",
    "        align_img = alignments[0].data.cpu().numpy().T\n",
    "\n",
    "        figures.update(\n",
    "            {\n",
    "                \"alignment\": plot_alignment(align_img, output_fig=False),\n",
    "            }\n",
    "        )\n",
    "        return figures, audios\n",
    "\n",
    "    def train_log(\n",
    "        self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int\n",
    "    ):  # pylint: disable=no-self-use\n",
    "        \"\"\"Create visualizations and waveform examples.\n",
    "\n",
    "        For example, here you can plot spectrograms and generate sample sample waveforms from these spectrograms to\n",
    "        be projected onto Tensorboard.\n",
    "\n",
    "        Args:\n",
    "            ap (AudioProcessor): audio processor used at training.\n",
    "            batch (Dict): Model inputs used at the previous training step.\n",
    "            outputs (Dict): Model outputs generated at the previoud training step.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict, np.ndarray]: training plots and output waveform.\n",
    "        \"\"\"\n",
    "        figures, audios = self._log(self.ap, batch, outputs, \"train\")\n",
    "        logger.train_figures(steps, figures)\n",
    "        logger.train_audios(steps, audios, self.ap.sample_rate)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_step(self, batch: dict, criterion: nn.Module, optimizer_idx: int):\n",
    "        return self.train_step(batch, criterion, optimizer_idx)\n",
    "\n",
    "    def eval_log(self, batch: dict, outputs: dict, logger: \"Logger\", assets: dict, steps: int) -> None:\n",
    "        figures, audios = self._log(self.ap, batch, outputs, \"eval\")\n",
    "        logger.eval_figures(steps, figures)\n",
    "        logger.eval_audios(steps, audios, self.ap.sample_rate)\n",
    "\n",
    "    def get_aux_input_from_test_sentences(self, sentence_info):\n",
    "        if hasattr(self.config, \"model_args\"):\n",
    "            config = self.config.model_args\n",
    "        else:\n",
    "            config = self.config\n",
    "\n",
    "        # extract speaker and language info\n",
    "        text, speaker_name, style_wav, language_name = None, None, None, None\n",
    "\n",
    "        if isinstance(sentence_info, list):\n",
    "            if len(sentence_info) == 1:\n",
    "                text = sentence_info[0]\n",
    "            elif len(sentence_info) == 2:\n",
    "                text, speaker_name = sentence_info\n",
    "            elif len(sentence_info) == 3:\n",
    "                text, speaker_name, style_wav = sentence_info\n",
    "            elif len(sentence_info) == 4:\n",
    "                text, speaker_name, style_wav, language_name = sentence_info\n",
    "        else:\n",
    "            text = sentence_info\n",
    "\n",
    "        # get speaker  id/d_vector\n",
    "        speaker_id, d_vector, language_id = None, None, None\n",
    "        if hasattr(self, \"speaker_manager\"):\n",
    "            if config.use_d_vector_file:\n",
    "                if speaker_name is None:\n",
    "                    d_vector = self.speaker_manager.get_random_embedding()\n",
    "                else:\n",
    "                    d_vector = self.speaker_manager.get_mean_embedding(speaker_name, num_samples=None, randomize=False)\n",
    "            elif config.use_speaker_embedding:\n",
    "                if speaker_name is None:\n",
    "                    speaker_id = self.speaker_manager.get_random_id()\n",
    "                else:\n",
    "                    speaker_id = self.speaker_manager.name_to_id[speaker_name]\n",
    "\n",
    "        # get language id\n",
    "        if hasattr(self, \"language_manager\") and config.use_language_embedding and language_name is not None:\n",
    "            language_id = self.language_manager.name_to_id[language_name]\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"speaker_id\": speaker_id,\n",
    "            \"style_wav\": style_wav,\n",
    "            \"d_vector\": d_vector,\n",
    "            \"language_id\": language_id,\n",
    "            \"language_name\": language_name,\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_run(self, assets) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"Generic test run for `tts` models used by `Trainer`.\n",
    "\n",
    "        You can override this for a different behaviour.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict, Dict]: Test figures and audios to be projected to Tensorboard.\n",
    "        \"\"\"\n",
    "        print(\" | > Synthesizing test sentences.\")\n",
    "        test_audios = {}\n",
    "        test_figures = {}\n",
    "        test_sentences = self.config.test_sentences\n",
    "        for idx, s_info in enumerate(test_sentences):\n",
    "            aux_inputs = self.get_aux_input_from_test_sentences(s_info)\n",
    "            wav, alignment, _, _ = synthesis(\n",
    "                self,\n",
    "                aux_inputs[\"text\"],\n",
    "                self.config,\n",
    "                \"cuda\" in str(next(self.parameters()).device),\n",
    "                speaker_id=aux_inputs[\"speaker_id\"],\n",
    "                d_vector=aux_inputs[\"d_vector\"],\n",
    "                style_wav=aux_inputs[\"style_wav\"],\n",
    "                language_id=aux_inputs[\"language_id\"],\n",
    "                use_griffin_lim=True,\n",
    "                do_trim_silence=False,\n",
    "            ).values()\n",
    "            test_audios[\"{}-audio\".format(idx)] = wav\n",
    "            test_figures[\"{}-alignment\".format(idx)] = plot_alignment(alignment.T, output_fig=False)\n",
    "        return {\"figures\": test_figures, \"audios\": test_audios}\n",
    "\n",
    "    def test_log(\n",
    "        self, outputs: dict, logger: \"Logger\", assets: dict, steps: int  # pylint: disable=unused-argument\n",
    "    ) -> None:\n",
    "        logger.test_audios(steps, outputs[\"audios\"], self.ap.sample_rate)\n",
    "        logger.test_figures(steps, outputs[\"figures\"])\n",
    "\n",
    "    def format_batch(self, batch: Dict) -> Dict:\n",
    "        \"\"\"Compute speaker, langugage IDs and d_vector for the batch if necessary.\"\"\"\n",
    "        speaker_ids = None\n",
    "        language_ids = None\n",
    "        d_vectors = None\n",
    "\n",
    "        # get numerical speaker ids from speaker names\n",
    "        if self.speaker_manager is not None and self.speaker_manager.name_to_id and self.args.use_speaker_embedding:\n",
    "            speaker_ids = [self.speaker_manager.name_to_id[sn] for sn in batch[\"speaker_names\"]]\n",
    "\n",
    "        if speaker_ids is not None:\n",
    "            speaker_ids = torch.LongTensor(speaker_ids)\n",
    "\n",
    "        # get d_vectors from audio file names\n",
    "        if self.speaker_manager is not None and self.speaker_manager.embeddings and self.args.use_d_vector_file:\n",
    "            d_vector_mapping = self.speaker_manager.embeddings\n",
    "            d_vectors = [d_vector_mapping[w][\"embedding\"] for w in batch[\"audio_unique_names\"]]\n",
    "            d_vectors = torch.FloatTensor(d_vectors)\n",
    "\n",
    "        # get language ids from language names\n",
    "        if self.language_manager is not None and self.language_manager.name_to_id and self.args.use_language_embedding:\n",
    "            language_ids = [self.language_manager.name_to_id[ln] for ln in batch[\"language_names\"]]\n",
    "\n",
    "        if language_ids is not None:\n",
    "            language_ids = torch.LongTensor(language_ids)\n",
    "\n",
    "        batch[\"language_ids\"] = language_ids\n",
    "        batch[\"d_vectors\"] = d_vectors\n",
    "        batch[\"speaker_ids\"] = speaker_ids\n",
    "        return batch\n",
    "\n",
    "    def format_batch_on_device(self, batch):\n",
    "        \"\"\"Compute spectrograms on the device.\"\"\"\n",
    "        ac = self.config.audio\n",
    "\n",
    "        if self.args.encoder_sample_rate:\n",
    "            wav = self.audio_resampler(batch[\"waveform\"])\n",
    "        else:\n",
    "            wav = batch[\"waveform\"]\n",
    "\n",
    "        # compute spectrograms\n",
    "        batch[\"spec\"] = wav_to_spec(wav, ac.fft_size, ac.hop_length, ac.win_length, center=False)\n",
    "\n",
    "        if self.args.encoder_sample_rate:\n",
    "            # recompute spec with high sampling rate to the loss\n",
    "            spec_mel = wav_to_spec(batch[\"waveform\"], ac.fft_size, ac.hop_length, ac.win_length, center=False)\n",
    "            # remove extra stft frames if needed\n",
    "            if spec_mel.size(2) > int(batch[\"spec\"].size(2) * self.interpolate_factor):\n",
    "                spec_mel = spec_mel[:, :, : int(batch[\"spec\"].size(2) * self.interpolate_factor)]\n",
    "            else:\n",
    "                batch[\"spec\"] = batch[\"spec\"][:, :, : int(spec_mel.size(2) / self.interpolate_factor)]\n",
    "        else:\n",
    "            spec_mel = batch[\"spec\"]\n",
    "\n",
    "        batch[\"mel\"] = spec_to_mel(\n",
    "            spec=spec_mel,\n",
    "            n_fft=ac.fft_size,\n",
    "            num_mels=ac.num_mels,\n",
    "            sample_rate=ac.sample_rate,\n",
    "            fmin=ac.mel_fmin,\n",
    "            fmax=ac.mel_fmax,\n",
    "        )\n",
    "\n",
    "        if self.args.encoder_sample_rate:\n",
    "            assert batch[\"spec\"].shape[2] == int(\n",
    "                batch[\"mel\"].shape[2] / self.interpolate_factor\n",
    "            ), f\"{batch['spec'].shape[2]}, {batch['mel'].shape[2]}\"\n",
    "        else:\n",
    "            assert batch[\"spec\"].shape[2] == batch[\"mel\"].shape[2], f\"{batch['spec'].shape[2]}, {batch['mel'].shape[2]}\"\n",
    "\n",
    "        # compute spectrogram frame lengths\n",
    "        batch[\"spec_lens\"] = (batch[\"spec\"].shape[2] * batch[\"waveform_rel_lens\"]).int()\n",
    "        batch[\"mel_lens\"] = (batch[\"mel\"].shape[2] * batch[\"waveform_rel_lens\"]).int()\n",
    "\n",
    "        if self.args.encoder_sample_rate:\n",
    "            assert (batch[\"spec_lens\"] - (batch[\"mel_lens\"] / self.interpolate_factor).int()).sum() == 0\n",
    "        else:\n",
    "            assert (batch[\"spec_lens\"] - batch[\"mel_lens\"]).sum() == 0\n",
    "\n",
    "        # zero the padding frames\n",
    "        batch[\"spec\"] = batch[\"spec\"] * sequence_mask(batch[\"spec_lens\"]).unsqueeze(1)\n",
    "        batch[\"mel\"] = batch[\"mel\"] * sequence_mask(batch[\"mel_lens\"]).unsqueeze(1)\n",
    "        return batch\n",
    "\n",
    "    def get_sampler(self, config: Coqpit, dataset: TTSDataset, num_gpus=1, is_eval=False):\n",
    "        weights = None\n",
    "        data_items = dataset.samples\n",
    "        if getattr(config, \"use_weighted_sampler\", False):\n",
    "            for attr_name, alpha in config.weighted_sampler_attrs.items():\n",
    "                print(f\" > Using weighted sampler for attribute '{attr_name}' with alpha '{alpha}'\")\n",
    "                multi_dict = config.weighted_sampler_multipliers.get(attr_name, None)\n",
    "                print(multi_dict)\n",
    "                weights, attr_names, attr_weights = get_attribute_balancer_weights(\n",
    "                    attr_name=attr_name, items=data_items, multi_dict=multi_dict\n",
    "                )\n",
    "                weights = weights * alpha\n",
    "                print(f\" > Attribute weights for '{attr_names}' \\n | > {attr_weights}\")\n",
    "\n",
    "        # input_audio_lenghts = [os.path.getsize(x[\"audio_file\"]) for x in data_items]\n",
    "\n",
    "        if weights is not None:\n",
    "            w_sampler = WeightedRandomSampler(weights, len(weights))\n",
    "            batch_sampler = BucketBatchSampler(\n",
    "                w_sampler,\n",
    "                data=data_items,\n",
    "                batch_size=config.eval_batch_size if is_eval else config.batch_size,\n",
    "                sort_key=lambda x: os.path.getsize(x[\"audio_file\"]),\n",
    "                drop_last=True,\n",
    "            )\n",
    "        else:\n",
    "            batch_sampler = None\n",
    "        # sampler for DDP\n",
    "        if batch_sampler is None:\n",
    "            batch_sampler = DistributedSampler(dataset) if num_gpus > 1 else None\n",
    "        else:  # If a sampler is already defined use this sampler and DDP sampler together\n",
    "            batch_sampler = (\n",
    "                DistributedSamplerWrapper(batch_sampler) if num_gpus > 1 else batch_sampler\n",
    "            )  # TODO: check batch_sampler with multi-gpu\n",
    "        return batch_sampler\n",
    "\n",
    "    def get_data_loader(\n",
    "        self,\n",
    "        config: Coqpit,\n",
    "        assets: Dict,\n",
    "        is_eval: bool,\n",
    "        samples: Union[List[Dict], List[List]],\n",
    "        verbose: bool,\n",
    "        num_gpus: int,\n",
    "        rank: int = None,\n",
    "    ) -> \"DataLoader\":\n",
    "        if is_eval and not config.run_eval:\n",
    "            loader = None\n",
    "        else:\n",
    "            # init dataloader\n",
    "            dataset = VitsDataset(\n",
    "                model_args=self.args,\n",
    "                samples=samples,\n",
    "                batch_group_size=0 if is_eval else config.batch_group_size * config.batch_size,\n",
    "                min_text_len=config.min_text_len,\n",
    "                max_text_len=config.max_text_len,\n",
    "                min_audio_len=config.min_audio_len,\n",
    "                max_audio_len=config.max_audio_len,\n",
    "                phoneme_cache_path=config.phoneme_cache_path,\n",
    "                precompute_num_workers=config.precompute_num_workers,\n",
    "                verbose=verbose,\n",
    "                tokenizer=self.tokenizer,\n",
    "                start_by_longest=config.start_by_longest,\n",
    "            )\n",
    "\n",
    "            # wait all the DDP process to be ready\n",
    "            if num_gpus > 1:\n",
    "                dist.barrier()\n",
    "\n",
    "            # sort input sequences from short to long\n",
    "            dataset.preprocess_samples()\n",
    "\n",
    "            # get samplers\n",
    "            sampler = self.get_sampler(config, dataset, num_gpus)\n",
    "            if sampler is None:\n",
    "                loader = DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=config.eval_batch_size if is_eval else config.batch_size,\n",
    "                    shuffle=False,  # shuffle is done in the dataset.\n",
    "                    collate_fn=dataset.collate_fn,\n",
    "                    drop_last=False,  # setting this False might cause issues in AMP training.\n",
    "                    num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n",
    "                    pin_memory=False,\n",
    "                )\n",
    "            else:\n",
    "                if num_gpus > 1:\n",
    "                    loader = DataLoader(\n",
    "                        dataset,\n",
    "                        sampler=sampler,\n",
    "                        batch_size=config.eval_batch_size if is_eval else config.batch_size,\n",
    "                        collate_fn=dataset.collate_fn,\n",
    "                        num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n",
    "                        pin_memory=False,\n",
    "                    )\n",
    "                else:\n",
    "                    loader = DataLoader(\n",
    "                        dataset,\n",
    "                        batch_sampler=sampler,\n",
    "                        collate_fn=dataset.collate_fn,\n",
    "                        num_workers=config.num_eval_loader_workers if is_eval else config.num_loader_workers,\n",
    "                        pin_memory=False,\n",
    "                    )\n",
    "        return loader\n",
    "\n",
    "    def get_optimizer(self) -> List:\n",
    "        \"\"\"Initiate and return the GAN optimizers based on the config parameters.\n",
    "        It returnes 2 optimizers in a list. First one is for the generator and the second one is for the discriminator.\n",
    "        Returns:\n",
    "            List: optimizers.\n",
    "        \"\"\"\n",
    "        # select generator parameters\n",
    "        optimizer0 = get_optimizer(self.config.optimizer, self.config.optimizer_params, self.config.lr_disc, self.disc)\n",
    "\n",
    "        gen_parameters = chain(params for k, params in self.named_parameters() if not k.startswith(\"disc.\"))\n",
    "        optimizer1 = get_optimizer(\n",
    "            self.config.optimizer, self.config.optimizer_params, self.config.lr_gen, parameters=gen_parameters\n",
    "        )\n",
    "        return [optimizer0, optimizer1]\n",
    "\n",
    "    def get_lr(self) -> List:\n",
    "        \"\"\"Set the initial learning rates for each optimizer.\n",
    "\n",
    "        Returns:\n",
    "            List: learning rates for each optimizer.\n",
    "        \"\"\"\n",
    "        return [self.config.lr_disc, self.config.lr_gen]\n",
    "\n",
    "    def get_scheduler(self, optimizer) -> List:\n",
    "        \"\"\"Set the schedulers for each optimizer.\n",
    "\n",
    "        Args:\n",
    "            optimizer (List[`torch.optim.Optimizer`]): List of optimizers.\n",
    "\n",
    "        Returns:\n",
    "            List: Schedulers, one for each optimizer.\n",
    "        \"\"\"\n",
    "        scheduler_D = get_scheduler(self.config.lr_scheduler_disc, self.config.lr_scheduler_disc_params, optimizer[0])\n",
    "        scheduler_G = get_scheduler(self.config.lr_scheduler_gen, self.config.lr_scheduler_gen_params, optimizer[1])\n",
    "        return [scheduler_D, scheduler_G]\n",
    "\n",
    "    def get_criterion(self):\n",
    "        \"\"\"Get criterions for each optimizer. The index in the output list matches the optimizer idx used in\n",
    "        `train_step()`\"\"\"\n",
    "        from TTS.tts.layers.losses import (  # pylint: disable=import-outside-toplevel\n",
    "            VitsDiscriminatorLoss,\n",
    "            VitsGeneratorLoss,\n",
    "        )\n",
    "\n",
    "        return [VitsDiscriminatorLoss(self.config), VitsGeneratorLoss(self.config)]\n",
    "\n",
    "    def load_checkpoint(\n",
    "        self, config, checkpoint_path, eval=False, strict=True, cache=False\n",
    "    ):  # pylint: disable=unused-argument, redefined-builtin\n",
    "        \"\"\"Load the model checkpoint and setup for training or inference\"\"\"\n",
    "        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n",
    "        # compat band-aid for the pre-trained models to not use the encoder baked into the model\n",
    "        # TODO: consider baking the speaker encoder into the model and call it from there.\n",
    "        # as it is probably easier for model distribution.\n",
    "        state[\"model\"] = {k: v for k, v in state[\"model\"].items() if \"speaker_encoder\" not in k}\n",
    "\n",
    "        if self.args.encoder_sample_rate is not None and eval:\n",
    "            # audio resampler is not used in inference time\n",
    "            self.audio_resampler = None\n",
    "\n",
    "        # handle fine-tuning from a checkpoint with additional speakers\n",
    "        if hasattr(self, \"emb_g\") and state[\"model\"][\"emb_g.weight\"].shape != self.emb_g.weight.shape:\n",
    "            num_new_speakers = self.emb_g.weight.shape[0] - state[\"model\"][\"emb_g.weight\"].shape[0]\n",
    "            print(f\" > Loading checkpoint with {num_new_speakers} additional speakers.\")\n",
    "            emb_g = state[\"model\"][\"emb_g.weight\"]\n",
    "            new_row = torch.randn(num_new_speakers, emb_g.shape[1])\n",
    "            emb_g = torch.cat([emb_g, new_row], axis=0)\n",
    "            state[\"model\"][\"emb_g.weight\"] = emb_g\n",
    "        # load the model weights\n",
    "        self.load_state_dict(state[\"model\"], strict=strict)\n",
    "\n",
    "        if eval:\n",
    "            self.eval()\n",
    "            assert not self.training\n",
    "\n",
    "    def load_fairseq_checkpoint(\n",
    "        self, config, checkpoint_dir, eval=False, strict=True\n",
    "    ):  # pylint: disable=unused-argument, redefined-builtin\n",
    "        \"\"\"Load VITS checkpoints released by fairseq here: https://github.com/facebookresearch/fairseq/tree/main/examples/mms\n",
    "        Performs some changes for compatibility.\n",
    "\n",
    "        Args:\n",
    "            config (Coqpit): ðŸ¸TTS model config.\n",
    "            checkpoint_dir (str): Path to the checkpoint directory.\n",
    "            eval (bool, optional): Set to True for evaluation. Defaults to False.\n",
    "        \"\"\"\n",
    "        import json\n",
    "\n",
    "        from TTS.tts.utils.text.cleaners import basic_cleaners\n",
    "\n",
    "        self.disc = None\n",
    "        # set paths\n",
    "        config_file = os.path.join(checkpoint_dir, \"config.json\")\n",
    "        checkpoint_file = os.path.join(checkpoint_dir, \"G_100000.pth\")\n",
    "        vocab_file = os.path.join(checkpoint_dir, \"vocab.txt\")\n",
    "        # set config params\n",
    "        with open(config_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            # Load the JSON data as a dictionary\n",
    "            config_org = json.load(file)\n",
    "        self.config.audio.sample_rate = config_org[\"data\"][\"sampling_rate\"]\n",
    "        # self.config.add_blank = config['add_blank']\n",
    "        # set tokenizer\n",
    "        vocab = FairseqVocab(vocab_file)\n",
    "        self.text_encoder.emb = nn.Embedding(vocab.num_chars, config.model_args.hidden_channels)\n",
    "        self.tokenizer = TTSTokenizer(\n",
    "            use_phonemes=False,\n",
    "            text_cleaner=basic_cleaners,\n",
    "            characters=vocab,\n",
    "            phonemizer=None,\n",
    "            add_blank=config_org[\"data\"][\"add_blank\"],\n",
    "            use_eos_bos=False,\n",
    "        )\n",
    "        # load fairseq checkpoint\n",
    "        new_chk = rehash_fairseq_vits_checkpoint(checkpoint_file)\n",
    "        self.load_state_dict(new_chk, strict=strict)\n",
    "        if eval:\n",
    "            self.eval()\n",
    "            assert not self.training\n",
    "\n",
    "    @staticmethod\n",
    "    def init_from_config(config: \"VitsConfig\", samples: Union[List[List], List[Dict]] = None, verbose=True):\n",
    "        \"\"\"Initiate model from config\n",
    "\n",
    "        Args:\n",
    "            config (VitsConfig): Model config.\n",
    "            samples (Union[List[List], List[Dict]]): Training samples to parse speaker ids for training.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "        from TTS.utils.audio import AudioProcessor\n",
    "\n",
    "        upsample_rate = torch.prod(torch.as_tensor(config.model_args.upsample_rates_decoder)).item()\n",
    "\n",
    "        if not config.model_args.encoder_sample_rate:\n",
    "            assert (\n",
    "                upsample_rate == config.audio.hop_length\n",
    "            ), f\" [!] Product of upsample rates must be equal to the hop length - {upsample_rate} vs {config.audio.hop_length}\"\n",
    "        else:\n",
    "            encoder_to_vocoder_upsampling_factor = config.audio.sample_rate / config.model_args.encoder_sample_rate\n",
    "            effective_hop_length = config.audio.hop_length * encoder_to_vocoder_upsampling_factor\n",
    "            assert (\n",
    "                upsample_rate == effective_hop_length\n",
    "            ), f\" [!] Product of upsample rates must be equal to the hop length - {upsample_rate} vs {effective_hop_length}\"\n",
    "\n",
    "        ap = AudioProcessor.init_from_config(config, verbose=verbose)\n",
    "        tokenizer, new_config = TTSTokenizer.init_from_config(config)\n",
    "        speaker_manager = SpeakerManager.init_from_config(config, samples)\n",
    "        language_manager = LanguageManager.init_from_config(config)\n",
    "\n",
    "        if config.model_args.speaker_encoder_model_path:\n",
    "            speaker_manager.init_encoder(\n",
    "                config.model_args.speaker_encoder_model_path, config.model_args.speaker_encoder_config_path\n",
    "            )\n",
    "        return Vits(new_config, ap, tokenizer, speaker_manager, language_manager)\n",
    "\n",
    "    def export_onnx(self, output_path: str = \"coqui_vits.onnx\", verbose: bool = True):\n",
    "        \"\"\"Export model to ONNX format for inference\n",
    "\n",
    "        Args:\n",
    "            output_path (str): Path to save the exported model.\n",
    "            verbose (bool): Print verbose information. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        # rollback values\n",
    "        _forward = self.forward\n",
    "        disc = None\n",
    "        if hasattr(self, \"disc\"):\n",
    "            disc = self.disc\n",
    "        training = self.training\n",
    "\n",
    "        # set export mode\n",
    "        self.disc = None\n",
    "        self.eval()\n",
    "\n",
    "        def onnx_inference(text, text_lengths, scales, sid=None, langid=None):\n",
    "            noise_scale = scales[0]\n",
    "            length_scale = scales[1]\n",
    "            noise_scale_dp = scales[2]\n",
    "            self.noise_scale = noise_scale\n",
    "            self.length_scale = length_scale\n",
    "            self.noise_scale_dp = noise_scale_dp\n",
    "            return self.inference(\n",
    "                text,\n",
    "                aux_input={\n",
    "                    \"x_lengths\": text_lengths,\n",
    "                    \"d_vectors\": None,\n",
    "                    \"speaker_ids\": sid,\n",
    "                    \"language_ids\": langid,\n",
    "                    \"durations\": None,\n",
    "                },\n",
    "            )[\"model_outputs\"]\n",
    "\n",
    "        self.forward = onnx_inference\n",
    "\n",
    "        # set dummy inputs\n",
    "        dummy_input_length = 100\n",
    "        sequences = torch.randint(low=0, high=2, size=(1, dummy_input_length), dtype=torch.long)\n",
    "        sequence_lengths = torch.LongTensor([sequences.size(1)])\n",
    "        scales = torch.FloatTensor([self.inference_noise_scale, self.length_scale, self.inference_noise_scale_dp])\n",
    "        dummy_input = (sequences, sequence_lengths, scales)\n",
    "        input_names = [\"input\", \"input_lengths\", \"scales\"]\n",
    "\n",
    "        if self.num_speakers > 0:\n",
    "            speaker_id = torch.LongTensor([0])\n",
    "            dummy_input += (speaker_id,)\n",
    "            input_names.append(\"sid\")\n",
    "\n",
    "        if hasattr(self, \"num_languages\") and self.num_languages > 0 and self.embedded_language_dim > 0:\n",
    "            language_id = torch.LongTensor([0])\n",
    "            dummy_input += (language_id,)\n",
    "            input_names.append(\"langid\")\n",
    "\n",
    "        # export to ONNX\n",
    "        torch.onnx.export(\n",
    "            model=self,\n",
    "            args=dummy_input,\n",
    "            opset_version=15,\n",
    "            f=output_path,\n",
    "            verbose=verbose,\n",
    "            input_names=input_names,\n",
    "            output_names=[\"output\"],\n",
    "            dynamic_axes={\n",
    "                \"input\": {0: \"batch_size\", 1: \"phonemes\"},\n",
    "                \"input_lengths\": {0: \"batch_size\"},\n",
    "                \"output\": {0: \"batch_size\", 1: \"time1\", 2: \"time2\"},\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # rollback\n",
    "        self.forward = _forward\n",
    "        if training:\n",
    "            self.train()\n",
    "        if not disc is None:\n",
    "            self.disc = disc\n",
    "\n",
    "    def load_onnx(self, model_path: str, cuda=False):\n",
    "        import onnxruntime as ort\n",
    "\n",
    "        providers = [\n",
    "            \"CPUExecutionProvider\"\n",
    "            if cuda is False\n",
    "            else (\"CUDAExecutionProvider\", {\"cudnn_conv_algo_search\": \"DEFAULT\"})\n",
    "        ]\n",
    "        sess_options = ort.SessionOptions()\n",
    "        self.onnx_sess = ort.InferenceSession(\n",
    "            model_path,\n",
    "            sess_options=sess_options,\n",
    "            providers=providers,\n",
    "        )\n",
    "\n",
    "    def inference_onnx(self, x, x_lengths=None, speaker_id=None, language_id=None):\n",
    "        \"\"\"ONNX inference\"\"\"\n",
    "\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.cpu().numpy()\n",
    "\n",
    "        if x_lengths is None:\n",
    "            x_lengths = np.array([x.shape[1]], dtype=np.int64)\n",
    "\n",
    "        if isinstance(x_lengths, torch.Tensor):\n",
    "            x_lengths = x_lengths.cpu().numpy()\n",
    "        scales = np.array(\n",
    "            [self.inference_noise_scale, self.length_scale, self.inference_noise_scale_dp],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        input_params = {\"input\": x, \"input_lengths\": x_lengths, \"scales\": scales}\n",
    "        if not speaker_id is None:\n",
    "            input_params[\"sid\"] = torch.tensor([speaker_id]).cpu().numpy()\n",
    "        if not language_id is None:\n",
    "            input_params[\"langid\"] = torch.tensor([language_id]).cpu().numpy()\n",
    "\n",
    "        audio = self.onnx_sess.run(\n",
    "            [\"output\"],\n",
    "            input_params,\n",
    "        )\n",
    "        return audio[0][0]\n",
    "\n",
    "\n",
    "##################################\n",
    "# VITS CHARACTERS\n",
    "##################################\n",
    "\n",
    "\n",
    "class VitsCharacters(BaseCharacters):\n",
    "    \"\"\"Characters class for VITs model for compatibility with pre-trained models\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        graphemes: str = _characters,\n",
    "        punctuations: str = _punctuations,\n",
    "        pad: str = _pad,\n",
    "        ipa_characters: str = _phonemes,\n",
    "    ) -> None:\n",
    "        if ipa_characters is not None:\n",
    "            graphemes += ipa_characters\n",
    "        super().__init__(graphemes, punctuations, pad, None, None, \"<BLNK>\", is_unique=False, is_sorted=True)\n",
    "\n",
    "    def _create_vocab(self):\n",
    "        self._vocab = [self._pad] + list(self._punctuations) + list(self._characters) + [self._blank]\n",
    "        self._char_to_id = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        # pylint: disable=unnecessary-comprehension\n",
    "        self._id_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n",
    "\n",
    "    @staticmethod\n",
    "    def init_from_config(config: Coqpit):\n",
    "        if config.characters is not None:\n",
    "            _pad = config.characters[\"pad\"]\n",
    "            _punctuations = config.characters[\"punctuations\"]\n",
    "            _letters = config.characters[\"characters\"]\n",
    "            _letters_ipa = config.characters[\"phonemes\"]\n",
    "            return (\n",
    "                VitsCharacters(graphemes=_letters, ipa_characters=_letters_ipa, punctuations=_punctuations, pad=_pad),\n",
    "                config,\n",
    "            )\n",
    "        characters = VitsCharacters()\n",
    "        new_config = replace(config, characters=characters.to_config())\n",
    "        return characters, new_config\n",
    "\n",
    "    def to_config(self) -> \"CharactersConfig\":\n",
    "        return CharactersConfig(\n",
    "            characters=self._characters,\n",
    "            punctuations=self._punctuations,\n",
    "            pad=self._pad,\n",
    "            eos=None,\n",
    "            bos=None,\n",
    "            blank=self._blank,\n",
    "            is_unique=False,\n",
    "            is_sorted=True,\n",
    "        )\n",
    "\n",
    "\n",
    "class FairseqVocab(BaseVocabulary):\n",
    "    def __init__(self, vocab: str):\n",
    "        super(FairseqVocab).__init__()\n",
    "        self.vocab = vocab\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        \"\"\"Return the vocabulary dictionary.\"\"\"\n",
    "        return self._vocab\n",
    "\n",
    "    @vocab.setter\n",
    "    def vocab(self, vocab_file):\n",
    "        with open(vocab_file, encoding=\"utf-8\") as f:\n",
    "            self._vocab = [x.replace(\"\\n\", \"\") for x in f.readlines()]\n",
    "        self.blank = self._vocab[0]\n",
    "        self.pad = \" \"\n",
    "        self._char_to_id = {s: i for i, s in enumerate(self._vocab)}  # pylint: disable=unnecessary-comprehension\n",
    "        self._id_to_char = {i: s for i, s in enumerate(self._vocab)}  # pylint: disable=unnecessary-comprehension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8645e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " | > Found 14903 files in /mnt/Stuff/TTS/speech_synthesis_in_bangla-master/resources/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: True\n",
      " | > Precision: fp16\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 12\n",
      " | > Num. of Torch Threads: 6\n",
      " | > Torch seed: 54321\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=/mnt/Stuff/TTS/speech_synthesis_in_bangla-master/run-October-31-2025_09+28AM-810fb35\n",
      "/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/trainer/trainer.py:552: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "\n",
      " > Model has 83046892 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/100\u001b[0m\n",
      " --> /mnt/Stuff/TTS/speech_synthesis_in_bangla-master/run-October-31-2025_09+28AM-810fb35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "> DataLoader initialization\n",
      "| > Tokenizer:\n",
      "\t| > add_blank: True\n",
      "\t| > use_eos_bos: False\n",
      "\t| > use_phonemes: False\n",
      "| > Number of instances : 14754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m > TRAINING (2025-10-31 09:28:49) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Preprocessing samples\n",
      " | > Max text length: 220\n",
      " | > Min text length: 1\n",
      " | > Avg text length: 68.42246170530026\n",
      " | \n",
      " | > Max audio length: 316417.0\n",
      " | > Min audio length: 18362.0\n",
      " | > Avg audio length: 108092.22407482717\n",
      " | > Num. instances discarded samples: 0\n",
      " | > Batch group size: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ! Run is removed from /mnt/Stuff/TTS/speech_synthesis_in_bangla-master/run-October-31-2025_09+28AM-810fb35\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/trainer/trainer.py\", line 1833, in fit\n",
      "    self._fit()\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/trainer/trainer.py\", line 1785, in _fit\n",
      "    self.train_epoch()\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/trainer/trainer.py\", line 1503, in train_epoch\n",
      "    for cur_step, batch in enumerate(self.train_loader):\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 732, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1506, in _next_data\n",
      "    return self._process_data(data, worker_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1541, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/torch/_utils.py\", line 769, in reraise\n",
      "    raise exception\n",
      "TypeError: Caught TypeError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
      "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/tmp/ipykernel_8518/2913128744.py\", line 325, in __getitem__\n",
      "    token_ids = self.get_token_ids(idx, raw_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/TTS/tts/datasets/dataset.py\", line 242, in get_token_ids\n",
      "    token_ids = self.tokenizer.text_to_ids(text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/TTS/tts/utils/text/tokenizer.py\", line 108, in text_to_ids\n",
      "    text = self.text_cleaner(text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/TTS/tts/utils/text/cleaners.py\", line 125, in phoneme_cleaners\n",
      "    text = en_normalize_numbers(text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/TTS/tts/utils/text/english/number_norm.py\", line 92, in normalize_numbers\n",
      "    text = re.sub(_comma_number_re, _remove_commas, text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/re/__init__.py\", line 185, in sub\n",
      "    return _compile(pattern, flags).sub(repl, string, count)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: expected string or bytes-like object, got 'Tensor'\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mushahid/anaconda3/envs/tts-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Trainer: Where the âœ¨ï¸ happens.\n",
    "# TrainingArgs: Defines the set of arguments of the Trainer.\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "# GlowTTSConfig: all model related values for training, validating and testing.\n",
    "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
    "from TTS.tts.configs.vits_config import VitsConfig\n",
    "\n",
    "# BaseDatasetConfig: defines name, formatter and path of the dataset.\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.glow_tts import GlowTTS\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "\n",
    "# we use the same path as this script as our training folder.\n",
    "output_path = os.path.dirname(os.path.abspath(\"/mnt/Stuff/TTS/speech_synthesis_in_bangla-master/output\"))\n",
    "\n",
    "def formatter(root_path, manifest_file, **kwargs):  # pylint: disable=unused-argument\n",
    "    \"\"\"Assumes each line as ```<filename>|<transcription>```\n",
    "    \"\"\"\n",
    "    txt_file = os.path.join(root_path, manifest_file)\n",
    "    items = []\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n",
    "        for line in ttf:\n",
    "            cols = line.split(\"|\")\n",
    "            wav_file = os.path.join(root_path, \"wavs\", cols[0])\n",
    "            text = cols[1]\n",
    "            items.append({\"text\":text, \"audio_file\":wav_file, \"speaker_name\":None, \"root_path\": root_path})\n",
    "    return items\n",
    "\n",
    "# DEFINE DATASET CONFIG\n",
    "# Set LJSpeech as our target dataset and define its path.\n",
    "# You can also use a simple Dict to define the dataset and pass it to your custom formatter.\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\", \n",
    "    meta_file_train=\"metadata.txt\", \n",
    "    path=os.path.join(output_path, \"/mnt/Stuff/TTS/speech_synthesis_in_bangla-master/resources/data/\")\n",
    ")\n",
    "\n",
    "# INITIALIZE THE TRAINING CONFIGURATION\n",
    "# Configure the model. Every config class inherits the BaseTTSConfig.\n",
    "config = VitsConfig(\n",
    "    batch_size=16,\n",
    "    eval_batch_size=16,\n",
    "    num_loader_workers=1,\n",
    "    num_eval_loader_workers=1,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    epochs=100,\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    use_phonemes=False,\n",
    "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
    "    print_step=25,\n",
    "    print_eval=False,\n",
    "    mixed_precision=True,\n",
    "    output_path=output_path,\n",
    "    datasets=[dataset_config],\n",
    ")\n",
    "\n",
    "# INITIALIZE THE AUDIO PROCESSOR\n",
    "# Audio processor is used for feature extraction and audio I/O.\n",
    "# It mainly serves to the dataloader and the training loggers.\n",
    "ap = AudioProcessor.init_from_config(config)\n",
    "\n",
    "# INITIALIZE THE TOKENIZER\n",
    "# Tokenizer is used to convert text to sequences of token IDs.\n",
    "# If characters are not defined in the config, default characters are passed to the config\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "\n",
    "# LOAD DATA SAMPLES\n",
    "# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n",
    "# You can define your custom sample loader returning the list of samples.\n",
    "# Or define your custom formatter and pass it to the `load_tts_samples`.\n",
    "# Check `TTS.tts.datasets.load_tts_samples` for more details.\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    dataset_config,\n",
    "    eval_split=True,\n",
    "    eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=config.eval_split_size,\n",
    "    formatter=formatter\n",
    ")\n",
    "\n",
    "# INITIALIZE THE MODEL\n",
    "# Models take a config object and a speaker manager as input\n",
    "# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n",
    "# Speaker manager is used by multi-speaker models.\n",
    "model = Vits(config, ap, tokenizer, speaker_manager=None)\n",
    "\n",
    "# INITIALIZE THE TRAINER\n",
    "# Trainer provides a generic API to train all the ðŸ¸TTS models with all its perks like mixed-precision training,\n",
    "# distributed training, etc.\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n",
    ")\n",
    "\n",
    "# AND... 3,2,1... ðŸš€\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8a124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
